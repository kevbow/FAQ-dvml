# Classification 2

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)

# scientific notation
options(scipen = 9999)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# libraries
library(tidyverse)
library(e1071)
library(tm)
library(katadasaR)
library(textclean)
library(rsample)
library(caret)
library(MLmetrics)
library(recipes)
```

## FAQ

**1. Dari berbagai metode klasifikasi yang telah dipelajari (logistic regression, KNN, naive bayes, decision tree, dan random forest), bagaimana pemilihan dalam penggunaan metode tersebut?**

   Pemilihan metode klasifikasi bergantung pada tujuan analisis yang dilakukan. Secara umum tujuan pemodelan klasifikasi adalah melakukan analisa terkait hubungan prediktor dengan target variabel atau melakukan prediksi.

   Jika tujuannya adalah untuk melakukan analisa terkait hubungan antara prediktor dan target variabel dapat menggunakan logistic regression atau decision tree. Berikut kelebihan dan kekurangan dari kedua metode tersebut.

   **Logistic regression:**

*  model klasifikasi yang cukup sederhana dan komputasinya cepat (+)
*  interpretable (+)
*  tidak mengharuskan scaling data (+)
*  baseline yang baik sebelum mencoba model yang lebih kompleks (+)
*  memerlukan ketelitian saat melakukan feature engineering karena sangat bergantung pada data yang fit
*  tidak dapat mengevaluasi hubungan yang tidak linier antara log of odds dan variabel prediktor(-)
*  mengharuskan antar prediktornya tidak saling terkait (cukup kaku) (-)

   **Decision tree:**

*  tidak mengharuskan scaling data (+)
*  dapat mengevaluasi hubungan yang tidak linier (+)
*  antar prediktornya boleh saling berkaitan (+)
*  interpretable (+)
*  decision tree yang terbentuk cenderung tidak stabil (sedikit perubahan pada data akan merubah struktur pohon yang dihasilkan) (-)
*  komputasi relatif lebih lama (-)
*  cenderung overfitting (-)

   Jika tujuannya adalah melakukan prediksi dengan harapan tingkat akurasi yang tinggi, bisa menggunakan random forest. Karena metode ini merupakan metode klasifikasi yang menggabungkan beberapa metode, sehingga cukup robust (tidak sensitif) terhadap outlier, antar prediktor boleh saling berkaitan, bahkan mengatasi overfitting. 

   Naive bayes umumnya digunakan untuk masalah-masalah yang berkaitan dengan klasifikasi text. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi [The Naive Bayes Classifier](https://towardsdatascience.com/the-naive-bayes-classifier-e92ea9f47523)

**2. Apakah k-fold cross validation dapat digunakan untuk metode klasifikasi lain (selain random forest)?**

  k-fold cross validation dapat digunakan untuk semua metode klasifikasi bahkan di luar metode klasifikasi yang telah dipelajari. Namun, karena k-fold cross validation tidak memperlihatkan hasil pemodelan untuk semua subset data (hanya mengambil yang terbaik, yaitu tingkat akurasi tertinggi), maka tetap perlu dilakukan cross validation untuk melakukan evaluasi model. Berikut contoh k-fold cross validation untuk metode lain (selain random forest).
```{r eval=FALSE}
set.seed(417)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
# parameter method dapat disesuaikan dengan metode klasifikasi yang digunakan
model <- train(attrition ~ ., data = train, method = "ctree", trControl = ctrl)
```

**3. Apakah pada metode KNN, naive bayes, decision tree, dan random forest hasilnya dapat berupa probability?**

   Pada dasarnya semua metode klasifikasi akan menghasilkan probability, bukan langsung kelas. Namun, kebanyakan metode klasifikasi secara default di R langsung menghasilkan kelas (threshold 0.5). Untuk menghasilkan probability dapat menambahkan parameter `type` saat melakukan `predict()`. Berikut beberapa `type` untuk metode klasifikasi yang dipelajari:

  -  `response` untuk logistic regression
  -  `raw` untuk naive bayes
  -  `probability` untuk decision tree dan random forest

**4. Apakah metode naive bayes dapat diterapkan untuk prediktor bertipe numerik? peluang apa yang dihitung?**

Naive bayes dapat diterapkan pada berbagai permasalahan klasifikasi bukan hanya pada klasifikasi text. Jika prediktor yang digunakan bertipe numerik naive bayes akan menghitung peluang rata-rata (mean) dan standard deviation (sd) untuk setiap level target. Berikut contoh naive bayes pada data `iris`.
```{r}
naiveBayes(Species ~ ., iris)
```

**5. Bagaimana cara menghapus `stopwords` dalam bahasa indonesia?**

File stopwords dapat di download terlebih dahulu di [link](https://github.com/stopwords-iso/stopwords-id) berikut. Kemudian import `stopwords id.txt` tersebut dengan menggunakan fungsi `readLines()`.
```{r}
# import Indonesia stopwords
stop_id <- readLines("data/05-C2/stopwords_id.txt")

# generate data frame
text <- data.frame(sentence = c("saya tertarik belajar data science di @algoritma :)",
                                "anda tinggal di Jakarta",
                                "Ingin ku merat🔥 na👍",
                                "selamat tahun baru #2020 !",
                                "pingin makan yang kek gitu"))
```

Mengubah text dalam bentuk data frame ke bentuk corpus dengan menggunakan fungsi `VectorSource()` dan `VCorpus()` dari library `tm`. Setelah itu, baru dapat menghapus stopwords dengan menggabungkan fungsi `tm_map()` dan `removeWords()`.
```{r}
text_clean1 <- text %>%
  pull(sentence) %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(removeWords, stop_id)

text_clean1[[1]]$content
```

**6. Bagaimana cara mengubah kata berimbuhan mejadi kata dasarnya saja dalam bahasa Indonesia?**

Untuk mengubah kata berimbuhan menjadi kata dasar dalam bahasa Indonesia dapat menggunakan fungsi `katadasaR()` dari library `katadasaR`. Namun, fungsi tersebut hanya menerima 1 inputan (1 value) saja sehigga dibutuhkan fungsi `sapply()` untuk mengaplikasikan fungsi tersebut ke dalam 1 kalimat.
```{r}
# membuat fungsi untuk mengubah kata berimbuhan menjadi kata dasar
kata_dasar <- function(x) {
  paste(sapply(words(x), katadasaR), collapse = " ")
  }
```

Menggunakan fungsi di atas dengan menggabungkan fungsi `tm_map()` dan `content_transformer()`.
```{r}
text_clean2 <- text %>%
  pull(sentence) %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(content_transformer(kata_dasar))

text_clean2[[1]]$content
```

**7. Bagaiamana cara menghapus emoticon dan emoji?**

Untuk menghapus emoticon dan emoji dapat menggunakan fungsi `replace_emoji()` dan `replace_emoticon()` dari library `textclean`. Namun, fungsi tersebut hanya menerima tipe data berupa karakter sehingga harus diubah terlebih dahulu tipe datanya jika masih belum karakter. 
```{r}
text_clean3 <- text %>%
  mutate(sentence = as.character(sentence)) %>% 
  pull(sentence) %>% 
  replace_emoji() %>% 
  replace_emoticon()

text_clean3
```

**8. Bagaimana cara menghapus mention dan hashtag?**

Untuk menghapus mention dan hashtag dapat menggunakan fungsi `replace_hash()` dan `replace_tag()` dari library `textclean`.
```{r}
text_clean4 <- text %>%
  mutate(sentence = as.character(sentence)) %>% 
  pull(sentence) %>% 
  replace_hash() %>% 
  replace_tag()

text_clean4
```

**9. Bagaimana cara menghapus slang words?**

Untuk menghapus slang words dapat menggunakan fungsi `replace_internet_slang()` dari library `textclean`.
```{r}
slang_id <- read.csv("data/05-C2/colloquial-indonesian-lexicon.csv") 

text_clean5 <- text %>%
  mutate(sentence = as.character(sentence)) %>% 
  pull(sentence) %>%
  replace_internet_slang(slang = paste0('\\b', slang_id$slang, '\\b') , 
                         replacement = slang_id$formal, 
                         ignore.case = T)

text_clean5
```

Berikut link eksternal yang dapat dijadikan sebagai bahan referensi dalam melakukan cleaning text [Text Cleaning Bahasa Indonesia](hthttps://algotech.netlify.com/blog/text-cleaning-bahasa/) dan [Text Cleaning Bahasa Inggris](https://algotech.netlify.com/blog/textclean/)

**10. Bagaimana penanganan terhadap data missing values?**

Terdapat berbagai cara menangani data yang mengandung missing value. Cara yang paling sering digunakan untuk menangani data yang mengandung missing value, yaitu deletion, full analysis, dan imputation.

- Deletion

Deletion adalah membuang variabel/kolom pada data yang memiliki jumlah missing value (`NA`) melebihi 50% dari jumlah observasi. Hal ini menganggap bahwa variabel tersebut tidak banyak memberikan informasi pada data (variansinya mendekati 0). Ketika membuang variabel perlu memperhatikan business case dari data tersebut, apakah variabel yang dibuang akan menghilangkan informasi yang cukup signfikan atau tidak? Apakah ketika variabel tersebut dibuang ada informasi yang berkurang dari data tersebut atau tidak?

- Full analysis

Full analysis adalah membuang observasi/baris yang mengandung missing value. Cara ini dilakukan jika jumlah observasi yang mengandung missing value `NA` tidak melebihi 5% dari total observasi data. 

Sebagai contoh akan digunakan data `credit_data`:
```{r}
load("data/05-C2/credit_data.RData")

head(credit_data, 5)

# cek missing value pada data
anyNA(credit_data)

# cek missing value terdapat pada variabel/kolom yang mana
colSums(is.na(credit_data))
```

Untuk menangani missing value pada data di atas dapat melakukan full analysis dengan menggunakan function `na.omit`:

```{r}
credit_full <- credit_data %>%
                na.omit()

colSums(is.na(credit_full))
```

Dari output di atas diketahui bahwa data `credit_data` sudah tidak mengandung missing value. Namun, jika diperhatikan jumlah observasi/baris pada data menjadi berkurang yang pada awalnya terdapat 4454 observasi setelah dilakukan full analysis menjadi 4039 observasi dengan variabel/kolom yang jumlahnya tetap sama.

- Imputation

Jika jumlah missing value pada data cukup banyak (melebihi 5% jumlah observasi), kita dapat melakukan imputation yaitu mengisi missing value tersebut dengan suatu nilai tertentu. Biasanya imputation dilakukan bedasarkan business knowledge dari variabel tersebut, misalkan variabel jumlah pengunjung per jam pada sebuah restoran. Seharusnya terdapat jumlah visitor yang bernilai 0 pada jam-jam tertentu karena pasti terdapat kemungkinan pada jam-jam tertentu tidak terdapat pengunjung. Tetapi, jika secara business knowledge seharusnya observasi tersebut memiliki suatu nilai, kita bisa melakukan imputation dengan menggunakan pusat datanya seperti mean/median untuk variabel numerik dan modus untuk variabel kategorik.

Sebelum melakukan imputation untuk mengisi data yang mengandung missing value terlebih dahulu dilakukan cross validation, yaitu membagi data menjadi **training set** untuk proses pemodelan dan **testing set** untuk melakukan evaluasi. Jika pada data test terdapat missing value, imputation akan dilakukan dengan memanfaatkan informasi dari data train. Data train dan data test tidak langsung dimasukkan ke dalam suatu objek melainkan dilakukan tahapan data pre-paration terlebih dahulu yang di dalamnya termasuk melakukan tahapan imputation. 

Cross validation akan dilakukan dengan menggunakan fungsi `initial_split()` dari library `rsample`. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode **stratified random sampling**, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set.
```{r}
set.seed(417)

splitted <- initial_split(data = credit_data, prop = 0.8, strata = "Status")

splitted
```

Imputation dilakukan dengan menggunakan Step Functions - Imputation dari library `recipes` yang didefinisikan dalam sebuah **recipe**.
```{r}
rec <- recipe(Status ~ ., training(splitted)) %>%
  # `step_meanimpute()` dapat diganti dengan Step Functions - Imputation lain
  step_meanimpute(Income, Assets, Debt) %>%
  prep()
```

Setelah mendefinisikan proses data preparation pada objek `rec`, selanjutnya proses tersebut diterapkan ke data train menggunakan fungsi `juice()` dan ke data test menggunakan fungsi `bake()` dari library `recipes`.
```{r}
# membuat data train dengan fungsi `juice()`
train <- juice(rec)

# membuat data test dengan fungsi `bake()`
test <- bake(rec, testing(splitted))
```

```{r}
colSums(is.na(train))
colSums(is.na(test))
```

Dari output di atas diketahui bahwa variabel `Income`, `Assets`, dan `Debt` sudah tidak mengandung missing value. 

- Link artikel mengenai missing value [6 Different Ways to Compensate for Missing Values In a Dataset (Data Imputation with examples)](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)

- Link mengenai Step Functions - Imputation dari library `recipes` [Step Functions - Imputation](https://tidymodels.github.io/recipes/reference/index.html#section-step-functions-imputation)

**11. Bagaimana implementasi model regresi pada random forest?**

import data yang akan digunakan
```{r}
insurance <- read.csv("data/05-C2/insurance.csv")
head(insurance)
```

Lakukan cross validation
```{r}
set.seed(100)
idx <- initial_split(insurance, prop = 0.8)

# check train dataset
train <- training(idx) 

# check test dataset
test <- testing(idx) 
```

Membuat model regresi dengan random forest tidak berbeda dengan kasus klasifikasi, ketika target variabel yang digunakan bertipe numerik, otomatis model akan menghasilkan model regresi.
```{r}
set.seed(100)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=3)
insurance_forest <- train(charges ~ ., data=train, method="rf", trControl = ctrl)
insurance_forest
```


Melakukan prediksi pada data test dan menghitung nilai RMSE
```{r}
test$pred <- predict(object = insurance_forest,newdata = test)
RMSE(y_pred = test$pred,y_true = test$charges)
```

**12. Jelaskan kegunaan dari ROC dan AUC?**

  Kurva ROC (Receiver Operating Characteristic) menggambarkan seberapa baik kinerja model klasifikasi biner. Kurva ROC dibentuk dari nilai TPR (True Positive Rate) dan FPR (False Positive Rate) untuk semua nilai threshold dari 0 hingga 1. AUC (Area Under the Curve) adalah luas daerah dari kurva ROC. Nilai AUC mendekati 1 artinya model sangat baik, ketika nilai AUC berada di sekitar 0.5 maka model tersebut memiliki performance yang tidak baik dan cenderung hanya menebak secara acak.

**13. Apa yang dimaksud dengan ensemble method?**

  Ensemble method merupakan gabungan prediksi dari beberapa model menjadi prediksi tunggal. Random forest merupakan salah satu jenis ensemble method yang memanfaatkan konsep *Bagging*, dimana *Bagging* merupakan gabungan dari bootstrap dan aggregation. 

- Bootstrap merupakan proses pengambilan sampel dengan pengembalian, adanya pengembalian memiliki kemungkinan data yang diambil berulang. Setiap observasi memiliki peluang yang sama untuk dijadikan sampel.

- Aggregation, dari beberapa model yang telah terbentuk dikumpulkan semua hasil prediksi untuk menentukan hasil prediksi tunggal. Untuk klasifikasi, maka dilakukan `majority voting` dimana kelas yang paling banyak diprediksi akan menjadi targetnya. Sedangkan untuk regresi akan diperoleh nilai rata-rata targetnya dari setiap model.  


## Mathematics Concept

**Ilustrasi Bayes Theorem**

```{r echo=F}
knitr::include_graphics("assets/09-C2/bayes.PNG")
```
```{r echo = F}
knitr::include_graphics("assets/09-C2/bayes2.PNG")
```



**Independent Event**

Ketika ada 2 kejadian yang terjadi secara bersamaan, peluang satu kejadian tidak mempengaruhi kejadian yang lain. Maka, peluang terjadi 2 kejadian yang tidak saling berhubungan adalah hasil perkalian masing-masing peluang kejadian tersebut.

$$P(A \cap B) = P(A) \ P(B)$$

**Dependent Event**

Peluang satu kejadian dipengaruhi atau berubah sesuai dengan informasi tentang kejadian lainnya. Untuk menghitung peluangnya, kita bisa menggunakan Bayes Theorem.

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)} = \frac{P(B|A) P(A)}{P(B|A) P(A)\ +\  P(B|\neg A) P(\neg A)}$$

Dimana:

- $P(A|B)$ = Peluang terjadi A apabila diketahui B telah terjadi.
- $P(B|A)$ = Peluang terjadi B apabila diketahui A telah terjadi.
- $P(B|\neg A)$ = Peluang terjadi B apabila diketahui A tidak terjadi.
- $P(A)$ = Peluang terjadi A
- $P(\neg A)$ = Peluang tidak terjadi A

**Entropy**

Entropy adalah ukuran ketidakteraturan (measure of disorder) yang bisa digunakan untuk mewakili seberapa beragam kelas yang ada dalam suatu variabel.

$$Entropy = \Sigma_{i=1}^c -p_i \ log_2 \ p_i$$

* $p_i$: proporsi kelas ke-*i* (jumlah observasi kelas *i* dibagi total seluruh observasi)

Nilai entropy apabila dalam satu variabel terdapat 2 kelas atau nilai:

$$Entropy = -\ p_1 \ log_2 \ p_1 -p_2 \ log_2 \ p_2$$

**Information Gain**

Information Gain digunakan untuk mengukur perubahan Entropy dan tingkat keragaman kelas setelah dilakukan percabangan. Ketika kita memisahkan 1 data menjadi 2 cabang menggunakan variabel tertentu, information gain dipakai untuk menentukan variabel mana yang dapat memberikan penurunan Entropy yang paling besar.

$$Information \ Gain = Entropy(awal) - (P_1 \  Entropy_1 + P_2 \  Entropy_2)$$

Untuk mencari variabel terbaik yang bisa digunakan untuk memisahkan dua kelas supaya entropy-nya semakin kecil, kita cari nilai Information Gain untuk tiap variabel dan pilih yang memberikan Information Gain terbesar.

**Gini Index**

$$Gini = \Sigma_{i=1}^C\ p(i)\ (1-p(i))$$

Gini untuk 2 kelas
$$Gini =  p(a)\ (1-p(a))\ +\  p(b)\ (1-p(b))$$

Variable Importance yang dihitung oleh Random Forest didapatkan dari rumus Gini Importance, yang konsepnya sama dengan Information Gain pada Entropy.

$$Gini\ Importance = Gini_{awal} - (P_1\ Gini_1 + P_2\ Gini_2)$$

**ROC**

Berikut ini ilustrasi yang menggambarkan kondisi ROC (Receiver Operating characteristic Curve) yang diinginkan ketika kelas imbalance adalah mendapatkan *True Positive* setinggi mungkin dan *False Positive* serendah mungkin.

```{r echo=F}
knitr::include_graphics("assets/09-C2/roc-intuition.png")
```

