# Classification 2




```r
# libraries
library(tidyverse)
library(e1071)
library(tm)
library(katadasaR)
library(textclean)
library(rsample)
library(caret)
library(MLmetrics)
library(recipes)
```

1. Dari berbagai metode klasifikasi yang telah dipelajari (logistic regression, KNN, naive bayes, decision tree, dan random forest), bagaimana pemilihan dalam penggunaan metode tersebut?

   Pemilihan metode klasifikasi bergantung pada tujuan analisis yang dilakukan. Secara umum tujuan pemodelan klasifikasi adalah melakukan analisa terkait hubungan prediktor dengan target variabel atau melakukan prediksi.

   Jika tujuannya adalah untuk melakukan analisa terkait hubungan antara prediktor dan target variabel dapat menggunakan logistic regression atau decision tree. Berikut kelebihan dan kekurangan dari kedua metode tersebut.

   **Logistic regression:**

*  model klasifikasi yang cukup sederhana dan komputasinya cepat (+)
*  interpretable (+)
*  tidak mengharuskan scaling data (+)
*  baseline yang baik sebelum mencoba model yang lebih kompleks (+)
*  memerlukan ketelitian saat melakukan feature engineering karena sangat bergantung pada data yang fit
*  tidak dapat mengevaluasi hubungan yang tidak linier antara log of odds dan variabel prediktor(-)
*  mengharuskan antar prediktornya tidak saling terkait (cukup kaku) (-)

   **Decision tree:**

*  tidak mengharuskan scaling data (+)
*  dapat mengevaluasi hubungan yang tidak linier (+)
*  antar prediktornya boleh saling berkaitan (+)
*  interpretable (+)
*  decision tree yang terbentuk cenderung tidak stabil (sedikit perubahan pada data akan merubah struktur pohon yang dihasilkan) (-)
*  komputasi relatif lebih lama (-)
*  cenderung overfitting (-)

   Jika tujuannya adalah melakukan prediksi dengan harapan tingkat akurasi yang tinggi, bisa menggunakan random forest. Karena metode ini merupakan metode klasifikasi yang menggabungkan beberapa metode, sehingga cukup robust (tidak sensitif) terhadap outlier, antar prediktor boleh saling berkaitan, bahkan mengatasi overfitting. 

   Naive bayes umumnya digunakan untuk masalah-masalah yang berkaitan dengan klasifikasi text. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi [The Naive Bayes Classifier](https://towardsdatascience.com/the-naive-bayes-classifier-e92ea9f47523)

2. Apakah k-fold cross validation dapat digunakan untuk metode klasifikasi lain (selain random forest)?

  k-fold cross validation dapat digunakan untuk semua metode klasifikasi bahkan di luar metode klasifikasi yang telah dipelajari. Namun, karena k-fold cross validation tidak memperlihatkan hasil pemodelan untuk semua subset data (hanya mengambil yang terbaik, yaitu tingkat akurasi tertinggi), maka tetap perlu dilakukan cross validation untuk melakukan evaluasi model. Berikut contoh k-fold cross validation untuk metode lain (selain random forest).

```r
set.seed(417)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
# parameter method dapat disesuaikan dengan metode klasifikasi yang digunakan
model <- train(attrition ~ ., data = train, method = "ctree", trControl = ctrl)
```

3. Apakah pada metode KNN, naive bayes, decision tree, dan random forest hasilnya dapat berupa probability?

   Pada dasarnya semua metode klasifikasi akan menghasilkan probability, bukan langsung kelas. Namun, kebanyakan metode klasifikasi secara default di R langsung menghasilkan kelas (threshold 0.5). Untuk menghasilkan probability dapat menambahkan parameter `type` saat melakukan `predict()`. Berikut beberapa `type` untuk metode klasifikasi yang dipelajari:

  -  `response` untuk logistic regression
  -  `raw` untuk naive bayes
  -  `probability` untuk decision tree dan random forest

4. Apakah metode naive bayes dapat diterapkan untuk prediktor bertipe numerik? peluang apa yang dihitung?

Naive bayes dapat diterapkan pada berbagai permasalahan klasifikasi bukan hanya pada klasifikasi text. Jika prediktor yang digunakan bertipe numerik naive bayes akan menghitung peluang rata-rata (mean) dan standard deviation (sd) untuk setiap level target. Berikut contoh naive bayes pada data `iris`.

```r
naiveBayes(Species ~ ., iris)
```

```
#> 
#> Naive Bayes Classifier for Discrete Predictors
#> 
#> Call:
#> naiveBayes.default(x = X, y = Y, laplace = laplace)
#> 
#> A-priori probabilities:
#> Y
#>     setosa versicolor  virginica 
#>  0.3333333  0.3333333  0.3333333 
#> 
#> Conditional probabilities:
#>             Sepal.Length
#> Y             [,1]      [,2]
#>   setosa     5.006 0.3524897
#>   versicolor 5.936 0.5161711
#>   virginica  6.588 0.6358796
#> 
#>             Sepal.Width
#> Y             [,1]      [,2]
#>   setosa     3.428 0.3790644
#>   versicolor 2.770 0.3137983
#>   virginica  2.974 0.3224966
#> 
#>             Petal.Length
#> Y             [,1]      [,2]
#>   setosa     1.462 0.1736640
#>   versicolor 4.260 0.4699110
#>   virginica  5.552 0.5518947
#> 
#>             Petal.Width
#> Y             [,1]      [,2]
#>   setosa     0.246 0.1053856
#>   versicolor 1.326 0.1977527
#>   virginica  2.026 0.2746501
```

5. Bagaimana cara menghapus `stopwords` dalam bahasa indonesia?

File stopwords dapat di download terlebih dahulu di [link](https://github.com/stopwords-iso/stopwords-id) berikut. Kemudian import `stopwords id.txt` tersebut dengan menggunakan fungsi `readLines()`.

```r
# import Indonesia stopwords
stop_id <- readLines("data/03-C2/stopwords_id.txt")

# generate data frame
text <- data.frame(sentence = c("saya tertarik belajar data science di @algoritma :)",
                                "anda tinggal di Jakarta",
                                "Ingin ku merat🔥 na👍",
                                "selamat tahun baru #2020 !",
                                "pingin makan yang kek gitu"))
```

Mengubah text dalam bentuk data frame ke bentuk corpus dengan menggunakan fungsi `VectorSource()` dan `VCorpus()` dari library `tm`. Setelah itu, baru dapat menghapus stopwords dengan menggabungkan fungsi `tm_map()` dan `removeWords()`.

```r
text_clean1 <- text %>%
  pull(sentence) %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(removeWords, stop_id)

text_clean1[[1]]$content
```

```
#> [1] " tertarik belajar data science  @algoritma :)"
```


6. Bagaimana cara mengubah kata berimbuhan mejadi kata dasarnya saja dalam bahasa Indonesia?

Untuk mengubah kata berimbuhan menjadi kata dasar dalam bahasa Indonesia dapat menggunakan fungsi `katadasaR()` dari library `katadasaR`. Namun, fungsi tersebut hanya menerima 1 inputan (1 value) saja sehigga dibutuhkan fungsi `sapply()` untuk mengaplikasikan fungsi tersebut ke dalam 1 kalimat.

```r
# membuat fungsi untuk mengubah kata berimbuhan menjadi kata dasar
kata_dasar <- function(x) {
  paste(sapply(words(x), katadasaR), collapse = " ")
  }
```

Menggunakan fungsi di atas dengan menggabungkan fungsi `tm_map()` dan `content_transformer()`.

```r
text_clean2 <- text %>%
  pull(sentence) %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(content_transformer(kata_dasar))

text_clean2[[1]]$content
```

```
#> [1] "saya tarik ajar data science di @algoritma :)"
```

7. Bagaiamana cara menghapus emoticon dan emoji?

Untuk menghapus emoticon dan emoji dapat menggunakan fungsi `replace_emoji()` dan `replace_emoticon()` dari library `textclean`. Namun, fungsi tersebut hanya menerima tipe data berupa karakter sehingga harus diubah terlebih dahulu tipe datanya jika masih belum karakter. 

```r
text_clean3 <- text %>%
  mutate(sentence = as.character(sentence)) %>% 
  pull(sentence) %>% 
  replace_emoji() %>% 
  replace_emoticon()

text_clean3
```

```
#> [1] "saya tertarik belajar data science di @algoritma smiley "
#> [2] "anda tinggal di Jakarta"                                 
#> [3] "Ingin ku merat<U+0001F525> na<U+0001F44D>"               
#> [4] "selamat tahun baru #2020 !"                              
#> [5] "pingin makan yang kek gitu"
```

8. Bagaimana cara menghapus mention dan hashtag?

Untuk menghapus mention dan hashtag dapat menggunakan fungsi `replace_hash()` dan `replace_tag()` dari library `textclean`.

```r
text_clean4 <- text %>%
  mutate(sentence = as.character(sentence)) %>% 
  pull(sentence) %>% 
  replace_hash() %>% 
  replace_tag()

text_clean4
```

```
#> [1] "saya tertarik belajar data science di  :)"
#> [2] "anda tinggal di Jakarta"                  
#> [3] "Ingin ku merat<U+0001F525> na<U+0001F44D>"
#> [4] "selamat tahun baru  !"                    
#> [5] "pingin makan yang kek gitu"
```

9. Bagaimana cara menghapus slang words?

Untuk menghapus slang words dapat menggunakan fungsi `replace_internet_slang()` dari library `textclean`.

```r
slang_id <- read.csv("data/03-C2/colloquial-indonesian-lexicon.csv") 

text_clean5 <- text %>%
  mutate(sentence = as.character(sentence)) %>% 
  pull(sentence) %>%
  replace_internet_slang(slang = paste0('\\b', slang_id$slang, '\\b') , 
                         replacement = slang_id$formal, 
                         ignore.case = T)

text_clean5
```

```
#> [1] "saya tertarik belajar data science di @algoritma :)"
#> [2] "anda tinggal di Jakarta"                            
#> [3] "Ingin ku merat<untuk+0001F525> nya<untuk+0001F44D>" 
#> [4] "selamat tahun baru #2020 !"                         
#> [5] "pengin makan yang kayak begitu"
```

Berikut link eksternal yang dapat dijadikan sebagai bahan referensi dalam melakukan cleaning text [Text Cleaning Bahasa Indonesia](hthttps://algotech.netlify.com/blog/text-cleaning-bahasa/) dan [Text Cleaning Bahasa Inggris](https://algotech.netlify.com/blog/textclean/)

10. Bagaimana penanganan terhadap data missing values?

Terdapat berbagai cara menangani data yang mengandung missing value. Cara yang paling sering digunakan untuk menangani data yang mengandung missing value, yaitu deletion, full analysis, dan imputation.

- Deletion

Deletion adalah membuang variabel/kolom pada data yang memiliki jumlah missing value (`NA`) melebihi 50% dari jumlah observasi. Hal ini menganggap bahwa variabel tersebut tidak banyak memberikan informasi pada data (variansinya mendekati 0). Ketika membuang variabel perlu memperhatikan business case dari data tersebut, apakah variabel yang dibuang akan menghilangkan informasi yang cukup signfikan atau tidak? Apakah ketika variabel tersebut dibuang ada informasi yang berkurang dari data tersebut atau tidak?

- Full analysis

Full analysis adalah membuang observasi/baris yang mengandung missing value. Cara ini dilakukan jika jumlah observasi yang mengandung missing value `NA` tidak melebihi 5% dari total observasi data. 

Sebagai contoh akan digunakan data `credit_data`:

```r
load("data/03-C2/credit_data.RData")

head(credit_data, 5)
```

```
#>   Status Seniority  Home Time Age Marital Records       Job Expenses Income
#> 1   good         9  rent   60  30 married      no freelance       73    129
#> 2   good        17  rent   60  58   widow      no     fixed       48    131
#> 3    bad        10 owner   36  46 married     yes freelance       90    200
#> 4   good         0  rent   60  24  single      no     fixed       63    182
#> 5   good         0  rent   36  26  single      no     fixed       46    107
#>   Assets Debt Amount Price
#> 1      0    0    800   846
#> 2      0    0   1000  1658
#> 3   3000    0   2000  2985
#> 4   2500    0    900  1325
#> 5      0    0    310   910
```

```r
# cek missing value pada data
anyNA(credit_data)
```

```
#> [1] TRUE
```

```r
# cek missing value terdapat pada variabel/kolom yang mana
colSums(is.na(credit_data))
```

```
#>    Status Seniority      Home      Time       Age   Marital   Records       Job 
#>         0         0         6         0         0         1         0         2 
#>  Expenses    Income    Assets      Debt    Amount     Price 
#>         0       381        47        18         0         0
```

Untuk menangani missing value pada data di atas dapat melakukan full analysis dengan menggunakan function `na.omit`:


```r
credit_full <- credit_data %>%
                na.omit()

colSums(is.na(credit_full))
```

```
#>    Status Seniority      Home      Time       Age   Marital   Records       Job 
#>         0         0         0         0         0         0         0         0 
#>  Expenses    Income    Assets      Debt    Amount     Price 
#>         0         0         0         0         0         0
```

Dari output di atas diketahui bahwa data `credit_data` sudah tidak mengandung missing value. Namun, jika diperhatikan jumlah observasi/baris pada data menjadi berkurang yang pada awalnya terdapat 4454 observasi setelah dilakukan full analysis menjadi 4039 observasi dengan variabel/kolom yang jumlahnya tetap sama.

- Imputation

Jika jumlah missing value pada data cukup banyak (melebihi 5% jumlah observasi), kita dapat melakukan imputation yaitu mengisi missing value tersebut dengan suatu nilai tertentu. Biasanya imputation dilakukan bedasarkan business knowledge dari variabel tersebut, misalkan variabel jumlah pengunjung per jam pada sebuah restoran. Seharusnya terdapat jumlah visitor yang bernilai 0 pada jam-jam tertentu karena pasti terdapat kemungkinan pada jam-jam tertentu tidak terdapat pengunjung. Tetapi, jika secara business knowledge seharusnya observasi tersebut memiliki suatu nilai, kita bisa melakukan imputation dengan menggunakan pusat datanya seperti mean/median untuk variabel numerik dan modus untuk variabel kategorik.

Sebelum melakukan imputation untuk mengisi data yang mengandung missing value terlebih dahulu dilakukan cross validation, yaitu membagi data menjadi **training set** untuk proses pemodelan dan **testing set** untuk melakukan evaluasi. Jika pada data test terdapat missing value, imputation akan dilakukan dengan memanfaatkan informasi dari data train. Data train dan data test tidak langsung dimasukkan ke dalam suatu objek melainkan dilakukan tahapan data pre-paration terlebih dahulu yang di dalamnya termasuk melakukan tahapan imputation. 

Cross validation akan dilakukan dengan menggunakan fungsi `initial_split()` dari library `rsample`. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode **stratified random sampling**, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set.

```r
set.seed(417)

splitted <- initial_split(data = credit_data, prop = 0.8, strata = "Status")

splitted
```

```
#> <3565/889/4454>
```

Imputation dilakukan dengan menggunakan Step Functions - Imputation dari library `recipes` yang didefinisikan dalam sebuah **recipe**.

```r
rec <- recipe(Status ~ ., training(splitted)) %>%
  # `step_meanimpute()` dapat diganti dengan Step Functions - Imputation lain
  step_meanimpute(Income, Assets, Debt) %>%
  prep()
```

Setelah mendefinisikan proses data preparation pada objek `rec`, selanjutnya proses tersebut diterapkan ke data train menggunakan fungsi `juice()` dan ke data test menggunakan fungsi `bake()` dari library `recipes`.

```r
# membuat data train dengan fungsi `juice()`
train <- juice(rec)

# membuat data test dengan fungsi `bake()`
test <- bake(rec, testing(splitted))
```


```r
colSums(is.na(train))
```

```
#> Seniority      Home      Time       Age   Marital   Records       Job  Expenses 
#>         0         5         0         0         1         0         1         0 
#>    Income    Assets      Debt    Amount     Price    Status 
#>         0         0         0         0         0         0
```

```r
colSums(is.na(test))
```

```
#> Seniority      Home      Time       Age   Marital   Records       Job  Expenses 
#>         0         1         0         0         0         0         1         0 
#>    Income    Assets      Debt    Amount     Price    Status 
#>         0         0         0         0         0         0
```

Dari output di atas diketahui bahwa variabel `Income`, `Assets`, dan `Debt` sudah tidak mengandung missing value. 

- Link artikel mengenai missing value [6 Different Ways to Compensate for Missing Values In a Dataset (Data Imputation with examples)](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)

- Link mengenai Step Functions - Imputation dari library `recipes` [Step Functions - Imputation](https://tidymodels.github.io/recipes/reference/index.html#section-step-functions-imputation)

11. Bagaimana implementasi model regresi pada random forest?

import data yang akan digunakan

```r
insurance <- read.csv("data/03-C2/insurance.csv")
head(insurance)
```

```
#>   age    sex    bmi children smoker    region   charges
#> 1  19 female 27.900        0    yes southwest 16884.924
#> 2  18   male 33.770        1     no southeast  1725.552
#> 3  28   male 33.000        3     no southeast  4449.462
#> 4  33   male 22.705        0     no northwest 21984.471
#> 5  32   male 28.880        0     no northwest  3866.855
#> 6  31 female 25.740        0     no southeast  3756.622
```

Lakukan cross validation

```r
set.seed(100)
idx <- initial_split(insurance, prop = 0.8)

# check train dataset
train <- training(idx) 

# check test dataset
test <- testing(idx) 
```

Membuat model regresi dengan random forest tidak berbeda dengan kasus klasifikasi, ketika target variabel yang digunakan bertipe numerik, otomatis model akan menghasilkan model regresi.

```r
set.seed(100)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=3)
insurance_forest <- train(charges ~ ., data=train, method="rf", trControl = ctrl)
insurance_forest
```

```
#> Random Forest 
#> 
#> 857 samples
#>   6 predictor
#> 
#> No pre-processing
#> Resampling: Cross-Validated (5 fold, repeated 3 times) 
#> Summary of sample sizes: 685, 686, 685, 687, 685, 685, ... 
#> Resampling results across tuning parameters:
#> 
#>   mtry  RMSE      Rsquared   MAE     
#>   2     5478.898  0.8335458  3708.914
#>   5     4822.793  0.8433103  2729.368
#>   8     4944.418  0.8365218  2827.233
#> 
#> RMSE was used to select the optimal model using the smallest value.
#> The final value used for the model was mtry = 5.
```


Melakukan prediksi pada data test dan menghitung nilai RMSE

```r
test$pred <- predict(object = insurance_forest,newdata = test)
RMSE(y_pred = test$pred,y_true = test$charges)
```

```
#> [1] 3891.623
```

12. Jelaskan kegunaan dari ROC dan AUC?

    Kurva ROC (Receiver Operating Characteristic) menggambarkan seberapa baik kinerja model klasifikasi biner. Kurva ROC dibentuk dari nilai TPR (True Positive Rate) dan FPR (False Positive Rate) untuk semua nilai threshold dari 0 hingga 1. AUC (Area Under the Curve) adalah luas daerah dari kurva ROC. Nilai AUC mendekati 1 artinya model sangat baik, ketika nilai AUC berada di sekitar 0.5 maka model tersebut memiliki performance yang tidak baik dan cenderung hanya menebak secara acak.

13. Apa yang dimaksud dengan ensemble method?

    Ensemble method merupakan gabungan prediksi dari beberapa model menjadi prediksi tunggal. Random forest merupakan salah satu jenis ensemble method yang memanfaatkan konsep *Bagging*, dimana *Bagging* merupakan gabungan dari bootstrap dan aggregation. 

- Bootstrap merupakan proses pengambilan sampel dengan pengembalian, adanya pengembalian memiliki kemungkinan data yang diambil berulang. Setiap observasi memiliki peluang yang sama untuk dijadikan sampel.

- Aggregation, dari beberapa model yang telah terbentuk dikumpulkan semua hasil prediksi untuk menentukan hasil prediksi tunggal. Untuk klasifikasi, maka dilakukan `majority voting` dimana kelas yang paling banyak diprediksi akan menjadi targetnya. Sedangkan untuk regresi akan diperoleh nilai rata-rata targetnya dari setiap model.  

