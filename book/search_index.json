[
["FAQ.html", "FAQ Pendahuluan Bab 1 Regression Model Bab 2 Classification 1 2.1 Imbalance Target Variable Bab 3 Classification 2 3.1 Deletion 3.2 Full analysis 3.3 Imputation Bab 4 Unsupervised Learning 4.1 PCA 4.2 Cluster Bab 5 Time Series and Forecasting Bab 6 Neural Network and Deep Learning", " FAQ Team Algoritma February 5, 2020 body { text-align: justify} Pendahuluan Algoritma menyediakan pelatihan untuk membantu para pekerja profesional maupun pelajar untuk mencapai keterampilan dasar dalam berbagi bidang data science yang terdiri dari: Data Visualization, Machine Learning, Data Modelling, Inferensial Statistik, dll. Kumpulan FAQ ini diproduksi oleh tim Algoritma untuk membantu para peserta Data Science Academy atau khalayak umum mengenai pertanyaan pertanyaan yang sering ditemukan mengenai topik machine learning. Bab 1 Regression Model Bagaimana penanganan data kategorik pada model linear regression? Dengan menggunakan function lm pada R otomatis akan mengubah tipe data kategorik menjadi dummy variabel. Dummy variable berfungsi untuk mengkuantitatifkan variabel yang bersifat kualitatif (kategorik). Dummy variabel hanya mempunyai dua nilai yaitu 1 dan 0. Dummy memiliki nilai 1 untuk salah satu kategori dan nol untuk kategori yang lain. Jika terdapat sebanyak k kategori untuk suatu prediktor maka akan ditransformasi menjadi k-1 dummy. Mengapa untuk asumsi normality yang harus berdistribusi normal adalah error/residual ? Jika residual berdistribusi normal, itu artinya residual cenderung berkumpul di titik sekitar 0, dapat dikatakan hasil prediksi tidak terlalu melenceng jauh dari data actual. Error yang tidak berdistribusi normal disebabkan oleh: distribusi target variabel memang tidak normal Model yang digunakan tidak cocok, misal hubungan antara prediktor dengan target tidak linier melainkan kudratik/eksponensial/dll walaupun target variabel memiliki distribusi normal. Selain itu, error harus berdistribusi normal terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval). Terdapat banyak data outlier Mengapa untuk asumsi normality yang harus berdistribusi normal adalah error/residual ? Karena ingin melihat kecocokan model. Error yang tidak berdistribusi normal disebabkan oleh: distribusi target variabel memang tidak normal Model yang digunakan tidak cocok, misal hubungan antara prediktor dengan target tidak linier melainkan kudratik/eksponensial/dll walaupun target variabel memiliki distribusi normal. Selain itu, error harus berdistribusi normal terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval). Untuk prediktor kategorik, bagaimana jika terdapat kategori yang tidak signifikan (p value &gt; alpha)? apakah prediktor tersebut masih dianggap signifikan mempengaruhi target? Untuk variabel kategorik ketika salah satu variabel signifikan, kita anggap levels lainnya juga signifikan. Pada fungsi lm sudah otomatis melakukan transformasi data kategorik dengan level pertama yang dijadikan basis. Bagaimana jika dilakukan reorder level (mengubah urutan level), apakah akan mengubah hasil pemodelan? Hasil pemodelan tidak akan berubah, mengubah urutan level hanya akan mengubah basis yang digunakan. Bagaimana jika terdapat prediktor yang tidak signifikan, tetapi secara bisnis seharusnya prediktor tersebut berpengaruh terhadap target? Ketika variabel yang kita gunakan tidak signifikan secara statistik, namun secara bisnis berpengaruh terhadap target yang dimiliki, kita akan tetap mempertahankan variabel tersebut, banyak faktor yang menyebabkan variabel tersebut tidak signikan, bisa jadi karena data yang dimiliki tidak cukup banyak, banyak data oulier, atau ragam data yang hanya sedikit. Mengapa perlu dilakukan pengecekkan asumsi pada metode regresi linier? Pengecekkan asumsi dilakukan terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval). Ketika semua asumsi terpenuhi model dapat dikatakan BLUE (Best Linear Unbiased Estimator). Jika sudah dilakukan berbagai alternatif untuk pemenuhan asumsi, namun masih terdapat asumsi yang tidak terpenuhi apa yang harus dilakukan? Ketika sudah dilakukan berbagai alternatif untuk memenuhi asumsi namun asumsi masih tidak terpenuhi, itu artinya data yang kita miliki tidak cocok menggunakan regresi linear, dapat dicoba dengan model lain. Bagaimana jika diperoleh nilai AIC negatif? AIC dapat bernilai negatif atupun positif yang disebabkan oleh nilai dari fungsi maksimum likelihood berikut: AIC = 2k − 2ln(L), dimana k merupakan jumlah parameter (jumlah prediktor dan intersep) dan L merupakan nilai dari fungsi maksimum likelihood. Namun, pada pemilihan model nilai AIC yang dilihat adalah nilai AIC yang sudah diabsolutkan. Sehingga tanda negatif/positif pada hasil AIC tidak berpengaruh dalam proses pemilihan model. Model yang dipilih adalah model yang memiliki nilai abolut AIC terkecil, hal ini mengindikasikan bahwa semakin sedikit model tersebut kehilangan informasi yang dibawa. Negative values for AIC Perbedaan dari R-squared dan Adjusted R-squared? R-squared memperhitungkan variasi dari semua variabel independen terhadap variabel dependen. Sehingga setiap penambahan variabel independen akan meningkatkan nilai R-squared. Sedangkan pada adjusted r-squared akan memperhitungan variasi dari variabel independen yang signifikan terhadap variabel dependen. Oleh karena itu, pada multiple linear regression disarankan untuk melihat nilai Adjusted R-squared. Apa itu outlier? Data observasi yang terlihat sangat berbeda jauh dari observasi-observasi lainnya dan muncul dalam bentuk nilai ekstrim baik untuk sebuah variabel tunggal atau kombinasi. Cara yang dapat dilakukan untuk tuning model regresi? Banyak cara yang dapat dilakukan untuk tuning model regresi, salah satunya adalah dengan deteksi outlier pada data observasi. Deteksi outlier dari data yang dimiliki, apakah dengan atau tanpa data outlier tersebut akan mengganggu perfomance model yang dimiliki. Untuk apa ada p-value di output regresi jika sebelumnya kita sudah melakukan uji korelasi? Uji korelasi pada preprocessing data dilakukan untuk melihat secara umum apakah variabel prediktor dan target terdapat hubungan kuat atau tidak. Sedangkan uji pvalue pada output regresi pada setiap variabel prediktor menyatakan apakah setiap variabel prediktor benar-benar mempengaruhi target secara statistik atau tidak. Uji statistik apa yang dapat digunakan untuk uji normalitas dengan lebih dari 5000 observasi? Uji normalitas dengan observasi yang lebih dari 5000 dapat menggunakan uji Kolmogorov Smirnov dengan code sebagai berikut: ks.test(model$residuals, &quot;pnorm&quot;, mean = mean(model$residuals), sd = sd(model$residuals)) Bagaimana jika target variabel yang dimiliki berupa bilangan diskrit, apakah bisa dilakukan analisis regresi? Untuk analisis regresi dengan target variabel berupa bilangan diskrit dapat menggunakan regressi poisson. Untuk detail lengkapnya dapat dilihat di link berikut: link Bab 2 Classification 1 # libraries library(tidyverse) library(rsample) library(caret) library(recipes) Pada klasifikasi penentuan kelas didasarkan pada peluang, bagaimana jika peluang yang diperoleh sama besar, misal pada kasus klasifikasi biner diperoleh peluang masuk ke kelas 1 adalah 0.5 dan peluang masuk ke kelas 0 adalah 0.5? Hal tersebut bergantung pada user yang menentukan threshold/batasan probability untuk masuk ke kelas 1 atau masuk ke kelas 0. Namun, pada umumnya jika diperoleh probability &gt;= 0.5 maka observasi tersebut akan masuk ke kelas 1. Untuk prediktor kategorik, bagaimana jika terdapat kategori yang tidak signifikan (p value &gt; alpha)? apakah prediktor tersebut masih dianggap signifikan mempengaruhi target? Untuk level yang menjadi basis akan dianggap signifikan, untuk level lainnya yang tidak signifikan artinya memang level tersebut tidak memberikan pengaruh terhadap target variabel. Solusi yang dapat dilakukan adalah bining (level tersebut dijadikan satu level yg mirip dan signifikan) atau menambahkan jumlah observasi pada level yang tidak signifikan tersebut. Pada fungsi lm sudah otomatis melakukan transformasi data kategorik dengan level pertama yang dijadikan basis. Bagaimana jika dilakukan reorder level (mengubah urutan level), apakah akan mengubah hasil pemodelan? Nilai pvalue pada setiap level tidak akan berubah ketika kita melakukan reorder level. Interpretasi untuk variable kategorik bergantung pada level yang dijadikan basis Pengertian dari null deviance dan residual deviance pada output summary? Null deviance menunjukkan seberapa baik target variabel diprediksi oleh model berdasarkan nilai intercept. Sedangkan residual deviance menunjukkan seberapa baik target variabel diprediksi oleh model berdasarkan intercept dan semua variabel independen yang digunakan. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi Null deviance &amp; Residual deviance Apa itu Fisher Scoring pada output summary? Fisher scoring adalah turunan dari metode Newton untuk mengatasi Maximum Likelihood. Fisher scoring memberikan informasi berapa banyak iterasi yang dilakukan pada model sehingga diperoleh nilai parameter. Apa itu Maximum Likelihood Estimator (MLE)? Parameter pada model logistik regression diperoleh dari pendekatan MLE. MLE merupakan pendekatan statistik untuk memperkirakan paramater pada model. Pada kasus klasifikasi, kenapa pengukuran accuracy tidak cukup menjelaskan seberapa baik model yang diperoleh? Untuk mengetahui seberapa baik perfomance model klasifikasi, tidak cukup dengan melihat nilai accuracy nya saja, karena accuracy menganggap sama penting untuk nilai false positive dan false negative. Kita membutuhkan perhitungan lain seperti precision dan recall, contohnya untuk memprediksi pasien mengidap kanker jinak atau ganas. Tentunya akan lebih berbahaya jika kemampuan model yang kita miliki cenderung lebih baik memprediksi kanker ganas namun terprediksi menjadi jinak. Pada kasus ini karena kelas positif yang dimiliki adalah kanker ganas, maka kita akan mementingkan nilai recall lebih besar dibandingkan nilai pengukuran lainnya. Permasalahan apa yang paling sering ditemui pada kasus klasifikasi? Permasalahan besar kasus klasifikasi adalah dataset yang tidak seimbang. Contohnya pada case churn telekomunikasi, employee attrition, prediksi kanker, fraud detection, dan sebagainya. Dalam hal tersebut, biasanya jumlah kelas positif jauh lebih sedikit dibandingkan kelas negatif. Misalkan pada kasus fraud detection, dari 1000 transaksi yang dimiliki, 10 diantaranya fraud, sedangkan sisanya tidak fraud. Kemudian perfomance model yang diperoleh sekitar 85%, mungkin terdengar sangat baik, namun pada kenyatannya tidak. Kemungkinan besar model tersebut hanya mampu memprediksi salah satu kelas kelas mayoritas yaitu yang tidak fraud, sedangkan kelas positif yang kita miliki sangat sedikit data yang dimiliki. 2.1 Imbalance Target Variable Pemodelan klasifikasi mulai banyak digunakan pada berbagai bidang industri, seperti perbankan untuk mendeteksi transaksi yang memiliki kecenderungan kecurangan atau memprediksi potensi kegagalan nasabah dalam membayar kredit/hutang, tranportasi (penerbangan) untuk memprediksi kemungkinan suatu penerbangan mengalami keterlambatan, digital marketing untuk memprediksi pelanggan yang loyal atau pelanggan yang memiliki potensi untuk kembali membeli produk yang dijual, kesehatan untuk memprediksi apakah seorang pasien positif terkena penyakit tertentu, dan masih banyak lagi. Dari berbagai macam permasalahan klasifikasi tersebut tidak semua masalah klasifikasi memiliki jumlah target variabel yang seimbang (level yang mendominasi keseluruhan target merupakan kelas mayoritas dan level yang lebih kecil disebut kelas minoritas). Ketika kondisi seperti apa target variabel tidak memiliki proporsi yang seimbang ? Hal tersebut akan berpengaruh terhadap kemampuan model untuk memprediksi target (model klasifikasi cenderung lebih pintar dalam memprediksi kelas mayoritas), karena model klasifikasi sangat bergantung pada jumlah setiap level target dalam proses learning nya (model klasifikasi akan melalui proses learning yang seimbang jika diberikan jumlah sampel yang seimbang pula). Hal ini menjadi masalah yang cukup serius, sehingga perlu dilakukan penanganan sebagai solusi permasalahan tersebut. attrition &lt;- read_csv(&quot;data/02-C1/attrition.csv&quot;) %&gt;% mutate(attrition = as.factor(attrition)) prop.table(table(attrition$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.84 0.16 Salah satu cara yang paling umum dilakukan adalah menyeimbangkan jumlah target variabel dengan metode sampling. Metode tersebut terbagi menjadi 2, yaitu downsampling dan upsampling. Downsampling adalah proses sampling pada observasi kelas mayoritas sebanyak jumlah observasi pada kelas minoritas, tujuannya adalah menyamakan jumlah observasi pada kelas mayoritas dan minoritas. Sehingga model klasifikasi dapat melalui proses learning yang seimbang. Proses downsampling akan mengurangi jumlah observasi pada kelas mayoritas, sehingga memungkinkan terjadinya kehilangan informasi. Upsampling adalah proses sampling pada observasi kelas minoritas sebanyak jumlah observasi pada kelas mayoritas, tujuannya adalah menyamakan jumlah observasi pada kelas mayoritas dan minoritas. Sehingga model klasifikasi dapat melalui proses learning yang seimbang. Proses upsampling akan menambah jumlah observasi pada kelas minoritas, sehingga memungkinkan terdapat data yang duplicate pada kelas minoritas. Untuk melakukan downsampling dan upsampling dapat menggunakan fungsi pada library caret ataupun recipes. Berikut contoh downsampling dan upsampling dengan menggunakan fungsi pada library caret dan recipes Sebelum menerapkan downsampling dan upsamling terlebih dahulu dilakukan cross validation, yaitu membagi data menjadi training set untuk proses pemodelan dan testing set untuk melakukan evaluasi. Cross validation akan dilakukan dengan menggunakan fungsi initial_split() dari library rsample. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode stratified random sampling, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set. # define seed set.seed(100) # menentukan indeks untuk train dan test splitted &lt;- initial_split(data = attrition, prop = 0.75, strata = &quot;attrition&quot;) # mengambil indeks data train dengan fungsi `tarining()` train &lt;- training(splitted) # mengambil indeks data test dengan fungsi `testing()` test &lt;- testing(splitted) prop.table(table(train$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.84 0.16 prop.table(table(test$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.84 0.16 Downsampling dan upsampling hanya akan dilakukan pada data train karena proses pembuatan model klasifikasi hanya dilakukan pada data train. Data test hanya digunakan untuk mengevaluasi model yang dihasilkan pada data train. 2.1.1 Downsample Untuk melakukan downsampling dengan library caret dapat menggunakan fungsi downSample(). train_down &lt;- downSample(x = train[, -1], y = train$attrition, yname = &quot;attrition&quot;) prop.table(table(train_down$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.5 0.5 2.1.2 Upsample Untuk melakukan upsampling dengan library caret dapat menggunakan fungsi upSample(). train_up &lt;- upSample(x = train[, -1], y = train$attrition, yname = &quot;attrition&quot;) prop.table(table(train_up$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.5 0.5 Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi downSample: Down- and Up-Sampling Imbalanced Data 2.1.3 Downsample/Upsample Using Recipes Sama seperti saat menggunakan fungsi pada library caret, ketika menggunakan fungsi dari library recipes juga harus dilakukan cross validation terlebih dahulu. Perbedaan ketika menggunakan fungsi dari library recipes data train dan data test tidak langsung dimasukkan ke dalam sebuah objek melainkan dilakukan downsampling atau upsampling terlebih dahulu. set.seed(417) splitted_rec &lt;- initial_split(data = attrition, prop = 0.8, strata = &quot;attrition&quot;) splitted_rec #&gt; &lt;1177/293/1470&gt; Untuk melakukan downsampling atau upsampling menggunakan library recipes dapat menggunakan fungsi step_downsample() atau step_upsample() yang didefinisikan dalam sebuah recipe. rec &lt;- recipe(attrition ~ ., training(splitted)) %&gt;% # `step_downsample()` dapat diganti dengan `step_upsample()` step_downsample(attrition, ratio = 1, seed = 100) %&gt;% prep() # membuat data train dengan fungsi `juice()` train_rec &lt;- juice(rec) # membuat data test dengan fungsi `bake()` test_rec &lt;- bake(rec, testing(splitted)) prop.table(table(train_rec$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.5 0.5 Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi tidymodels/recipes Apa yang dimaksud dengan false positive dan false negative? False positive adalah kasus dimana sisi negatif terprediksi sebagai positif. Contohnya, pasien terprediksi mengidap kanker ganas, namun data actual nya pasien tersebut mengidap kanker jinak. False negative adalah kasus dimana sisi positif terprediksi sebagai negatif. Contohnya, pasien teprediksi mengidap kanker jinak, namun data actual nya pasien tersebut mengidap kanker ganas. Bagaimana model regresi logistic regression menangani data kategorik? Sama seperti kasus linear regression, pada logistic regression akan mengubah variabel kategorik menjadi dummy variabel. Apa maksud dari nilai AIC? AIC (Akaike Information Criterion) menggambarkan seberapa banyak informasi yang hilang pada model tersebut. AIC biasa digunakan untuk membandingkan beberapa model, karena berbeda dengan R-squared yang memiliki range semakin mendekati 1 semakin baik, AIC tidak memiliki batasan, jadi kita perlu membandingkannya dengan model lain. Jelaskan kegunaan dari ROC dan AUC? Kurva ROC (Receiver Operating Characteristic) menggambarkan seberapa baik kinerja model klasifikasi biner. Kurva ROC dibentuk dari nilai TPR (True Positive Rate) dan FPR (False Positive Rate) untuk semua nilai threshold dari 0 hingga 1. AUC (Area Under the Curve) adalah luas daerah dari kurva ROC. Nilai AUC mendekati 1 artinya model sangat baik, ketika nilai AUC berada di sekitar 0.5 maka model tersebut memiliki performance yang tidak baik dan cenderung hanya menebak secara acak. Bab 3 Classification 2 # libraries library(tidyverse) library(e1071) library(tm) library(katadasaR) library(textclean) library(rsample) library(caret) library(recipes) Dari berbagai metode klasifikasi yang telah dipelajari (logistic regression, KNN, naive bayes, decision tree, dan random forest), bagaimana pemilihan dalam penggunaan metode tersebut? Pemilihan metode klasifikasi bergantung pada tujuan analisis yang dilakukan. Secara umum tujuan pemodelan klasifikasi adalah melakukan analisa terkait hubungan prediktor dengan target variabel atau melakukan prediksi. Jika tujuannya adalah untuk melakukan analisa terkait hubungan antara prediktor dan target variabel dapat menggunakan logistic regression atau decision tree. Berikut kelebihan dan kekurangan dari kedua metode tersebut. Logistic regression: model klasifikasi yang cukup sederhana dan komputasinya cepat (+) interpretable (+) tidak mengharuskan scaling data (+) baseline yang baik sebelum mencoba model yang lebih kompleks (+) memerlukan ketelitian saat melakukan feature engineering karena sangat bergantung pada data yang fit tidak dapat mengevaluasi hubungan yang tidak linier (-) mengharuskan antar prediktornya tidak saling terkait (cukup kaku) (-) Decision tree: tidak mengharuskan scaling data (+) dapat mengevaluasi hubungan yang tidak linier (+) antar prediktornya boleh saling berkaitan (+) interpretable (+) decision tree yang terbentuk cenderung tidak stabil (sedikit perubahan pada data akan merubah struktur pohon yang dihasilkan) (-) komputasi relatif lebih lama (-) cenderung overfitting (-) Jika tujuannya adalah melakukan prediksi dengan harapan tingkat akurasi yang tinggi, bisa menggunakan random forest. Karena metode ini merupakan metode klasifikasi yang menggabungkan beberapa metode, sehingga cukup robust (tidak sensitif) terhadap outlier, antar prediktor boleh saling berkaitan, bahkan mengatasi overfitting. Naive bayes umumnya digunakan untuk masalah-masalah yang berkaitan dengan klasifikasi text. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi The Naive Bayes Classifier Apakah k-fold cross validation dapat digunakan untuk metode klasifikasi lain (selain random forest)? k-fold cross validation dapat digunakan untuk semua metode klasifikasi bahkan di luar metode klasifikasi yang telah dipelajari. Namun, karena k-fold cross validation tidak memperlihatkan hasil pemodelan untuk semua subset data (hanya mengambil yang terbaik, yaitu tingkat akurasi tertinggi), maka tetap perlu dilakukan cross validation untuk melakukan evaluasi model. Berikut contoh k-fold cross validation untuk metode lain (selain random forest). set.seed(417) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3) # parameter method dapat disesuaikan dengan metode klasifikasi yang digunakan model &lt;- train(attrition ~ ., data = train, method = &quot;ctree&quot;, trControl = ctrl) Apakah pada metode KNN, naive bayes, decision tree, dan random forest hasilnya dapat berupa probability? Pada dasarnya semua metode klasifikasi akan menghasilkan probability, bukan langsung kelas. Namun, kebanyakan metode klasifikasi secara default di R langsung menghasilkan kelas (threshold 0.5). Untuk menghasilkan probability dapat menambahkan parameter type saat melakukan predict(). Berikut beberapa type untuk metode klasifikasi yang dipelajari: response untuk logistic regression raw untuk naive bayes probability untuk decision tree dan random forest Apakah metode naive bayes dapat diterapkan untuk prediktor bertipe numerik? peluang apa yang dihitung? Naive bayes dapat diterapkan pada berbagai permasalahan klasifikasi bukan hanya pada klasifikasi text. Jika prediktor yang digunakan bertipe numerik naive bayes akan menghitung peluang rata-rata (mean) dan standard deviation (sd) untuk setiap level target. Berikut contoh naive bayes pada data iris. naiveBayes(Species ~ ., iris) #&gt; #&gt; Naive Bayes Classifier for Discrete Predictors #&gt; #&gt; Call: #&gt; naiveBayes.default(x = X, y = Y, laplace = laplace) #&gt; #&gt; A-priori probabilities: #&gt; Y #&gt; setosa versicolor virginica #&gt; 0.3333333 0.3333333 0.3333333 #&gt; #&gt; Conditional probabilities: #&gt; Sepal.Length #&gt; Y [,1] [,2] #&gt; setosa 5.006 0.3524897 #&gt; versicolor 5.936 0.5161711 #&gt; virginica 6.588 0.6358796 #&gt; #&gt; Sepal.Width #&gt; Y [,1] [,2] #&gt; setosa 3.428 0.3790644 #&gt; versicolor 2.770 0.3137983 #&gt; virginica 2.974 0.3224966 #&gt; #&gt; Petal.Length #&gt; Y [,1] [,2] #&gt; setosa 1.462 0.1736640 #&gt; versicolor 4.260 0.4699110 #&gt; virginica 5.552 0.5518947 #&gt; #&gt; Petal.Width #&gt; Y [,1] [,2] #&gt; setosa 0.246 0.1053856 #&gt; versicolor 1.326 0.1977527 #&gt; virginica 2.026 0.2746501 Bagaimana cara menghapus stopwords dalam bahasa indonesia? File stopwords dapat di download terlebih dahulu di link berikut. Kemudian import stopwords id.txt tersebut dengan menggunakan fungsi readLines(). # import Indonesia stopwords stop_id &lt;- readLines(&quot;data/03-C2/stopwords_id.txt&quot;) # generate data frame text &lt;- data.frame(sentence = c(&quot;saya tertarik belajar data science di @algoritma :)&quot;, &quot;anda tinggal di Jakarta&quot;, &quot;Ingin ku merat🔥 na👍&quot;, &quot;selamat tahun baru #2020 !&quot;, &quot;pingin makan yang kek gitu&quot;)) Mengubah text dalam bentuk data frame ke bentuk corpus dengan menggunakan fungsi VectorSource() dan VCorpus() dari library tm. Setelah itu, baru dapat menghapus stopwords dengan menggabungkan fungsi tm_map() dan removeWords(). text_clean1 &lt;- text %&gt;% pull(sentence) %&gt;% VectorSource() %&gt;% VCorpus() %&gt;% tm_map(removeWords, stop_id) text_clean1[[1]]$content #&gt; [1] &quot; tertarik belajar data science @algoritma :)&quot; Bagaimana cara mengubah kata berimbuhan mejadi kata dasarnya saja dalam bahasa Indonesia? Untuk mengubah kata berimbuhan menjadi kata dasar dalam bahasa Indonesia dapat menggunakan fungsi katadasaR() dari library katadasaR. Namun, fungsi tersebut hanya menerima 1 inputan (1 value) saja sehigga dibutuhkan fungsi sapply() untuk mengaplikasikan fungsi tersebut ke dalam 1 kalimat. # membuat fungsi untuk mengubah kata berimbuhan menjadi kata dasar kata_dasar &lt;- function(x) { paste(sapply(words(x), katadasaR), collapse = &quot; &quot;) } Menggunakan fungsi di atas dengan menggabungkan fungsi tm_map() dan content_transformer(). text_clean2 &lt;- text %&gt;% pull(sentence) %&gt;% VectorSource() %&gt;% VCorpus() %&gt;% tm_map(content_transformer(kata_dasar)) text_clean2[[1]]$content #&gt; [1] &quot;saya tarik ajar data science di @algoritma :)&quot; Bagaiamana cara menghapus emoticon dan emoji? Untuk menghapus emoticon dan emoji dapat menggunakan fungsi replace_emoji() dan replace_emoticon() dari library textclean. Namun, fungsi tersebut hanya menerima tipe data berupa karakter sehingga harus diubah terlebih dahulu tipe datanya jika masih belum karakter. text_clean3 &lt;- text %&gt;% mutate(sentence = as.character(sentence)) %&gt;% pull(sentence) %&gt;% replace_emoji() %&gt;% replace_emoticon() text_clean3 #&gt; [1] &quot;saya tertarik belajar data science di @algoritma smiley &quot; #&gt; [2] &quot;anda tinggal di Jakarta&quot; #&gt; [3] &quot;Ingin ku merat&lt;U+0001F525&gt; na&lt;U+0001F44D&gt;&quot; #&gt; [4] &quot;selamat tahun baru #2020 !&quot; #&gt; [5] &quot;pingin makan yang kek gitu&quot; Bagaimana cara menghapus mention dan hashtag? Untuk menghapus mention dan hashtag dapat menggunakan fungsi replace_hash() dan replace_tag() dari library textclean. text_clean4 &lt;- text %&gt;% mutate(sentence = as.character(sentence)) %&gt;% pull(sentence) %&gt;% replace_hash() %&gt;% replace_tag() text_clean4 #&gt; [1] &quot;saya tertarik belajar data science di :)&quot; #&gt; [2] &quot;anda tinggal di Jakarta&quot; #&gt; [3] &quot;Ingin ku merat&lt;U+0001F525&gt; na&lt;U+0001F44D&gt;&quot; #&gt; [4] &quot;selamat tahun baru !&quot; #&gt; [5] &quot;pingin makan yang kek gitu&quot; Bagaimana cara menghapus slang words? Untuk menghapus slang words dapat menggunakan fungsi replace_internet_slang() dari library textclean. slang_id &lt;- read.csv(&quot;data/03-C2/colloquial-indonesian-lexicon.csv&quot;) text_clean5 &lt;- text %&gt;% mutate(sentence = as.character(sentence)) %&gt;% pull(sentence) %&gt;% replace_internet_slang(slang = paste0(&#39;\\\\b&#39;, slang_id$slang, &#39;\\\\b&#39;) , replacement = slang_id$formal, ignore.case = T) text_clean5 #&gt; [1] &quot;saya tertarik belajar data science di @algoritma :)&quot; #&gt; [2] &quot;anda tinggal di Jakarta&quot; #&gt; [3] &quot;Ingin ku merat&lt;untuk+0001F525&gt; nya&lt;untuk+0001F44D&gt;&quot; #&gt; [4] &quot;selamat tahun baru #2020 !&quot; #&gt; [5] &quot;pengin makan yang kayak begitu&quot; Berikut link eksternal yang dapat dijadikan sebagai bahan referensi dalam melakukan cleaning text Text Cleaning Bahasa Indonesia dan Text Cleaning Bahasa Inggris Bagaimana penanganan terhadap data missing values? Terdapat berbagai cara menangani data yang mengandung missing value. Cara yang paling sering digunakan untuk menangani data yang mengandung missing value, yaitu deletion, full analysis, dan imputation. 3.1 Deletion Deletion adalah membuang variabel/kolom pada data yang memiliki jumlah missing value (NA) melebihi 50% dari jumlah observasi. Hal ini menganggap bahwa variabel tersebut tidak banyak memberikan informasi pada data (variansinya mendekati 0). Ketika membuang variabel perlu memperhatikan business case dari data tersebut, apakah variabel yang dibuang akan menghilangkan informasi yang cukup signfikan atau tidak? Apakah ketika variabel tersebut dibuang ada informasi yang berkurang dari data tersebut atau tidak? 3.2 Full analysis Full analysis adalah membuang observasi/baris yang mengandung missing value. Cara ini dilakukan jika jumlah observasi yang mengandung missing value NA tidak melebihi 5% dari total observasi data. Sebagai contoh akan digunakan data credit_data: load(&quot;data/03-C2/credit_data.RData&quot;) head(credit_data, 5) #&gt; Status Seniority Home Time Age Marital Records Job Expenses Income #&gt; 1 good 9 rent 60 30 married no freelance 73 129 #&gt; 2 good 17 rent 60 58 widow no fixed 48 131 #&gt; 3 bad 10 owner 36 46 married yes freelance 90 200 #&gt; 4 good 0 rent 60 24 single no fixed 63 182 #&gt; 5 good 0 rent 36 26 single no fixed 46 107 #&gt; Assets Debt Amount Price #&gt; 1 0 0 800 846 #&gt; 2 0 0 1000 1658 #&gt; 3 3000 0 2000 2985 #&gt; 4 2500 0 900 1325 #&gt; 5 0 0 310 910 # cek missing value pada data anyNA(credit_data) #&gt; [1] TRUE # cek missing value terdapat pada variabel/kolom yang mana colSums(is.na(credit_data)) #&gt; Status Seniority Home Time Age Marital Records Job #&gt; 0 0 6 0 0 1 0 2 #&gt; Expenses Income Assets Debt Amount Price #&gt; 0 381 47 18 0 0 Untuk menangani missing value pada data di atas dapat melakukan full analysis dengan menggunakan function na.omit: credit_full &lt;- credit_data %&gt;% na.omit() colSums(is.na(credit_full)) #&gt; Status Seniority Home Time Age Marital Records Job #&gt; 0 0 0 0 0 0 0 0 #&gt; Expenses Income Assets Debt Amount Price #&gt; 0 0 0 0 0 0 Dari output di atas diketahui bahwa data credit_data sudah tidak mengandung missing value. Namun, jika diperhatikan jumlah observasi/baris pada data menjadi berkurang yang pada awalnya terdapat 4454 observasi setelah dilakukan full analysis menjadi 4039 observasi dengan variabel/kolom yang jumlahnya tetap sama. 3.3 Imputation Jika jumlah missing value pada data cukup banyak (melebihi 5% jumlah observasi), kita dapat melakukan imputation yaitu mengisi missing value tersebut dengan suatu nilai tertentu. Biasanya imputation dilakukan bedasarkan business knowledge dari variabel tersebut, misalkan variabel jumlah pengunjung per jam pada sebuah restoran. Seharusnya terdapat jumlah visitor yang bernilai 0 pada jam-jam tertentu karena pasti terdapat kemungkinan pada jam-jam tertentu tidak terdapat pengunjung. Tetapi, jika secara business knowledge seharusnya observasi tersebut memiliki suatu nilai, kita bisa melakukan imputation dengan menggunakan pusat datanya seperti mean/median untuk variabel numerik dan modus untuk variabel kategorik. Sebelum melakukan imputation untuk mengisi data yang mengandung missing value terlebih dahulu dilakukan cross validation, yaitu membagi data menjadi training set untuk proses pemodelan dan testing set untuk melakukan evaluasi. Jika pada data test terdapat missing value, imputation akan dilakukan dengan memanfaatkan informasi dari data train. Data train dan data test tidak langsung dimasukkan ke dalam suatu objek melainkan dilakukan tahapan data pre-paration terlebih dahulu yang di dalamnya termasuk melakukan tahapan imputation. Cross validation akan dilakukan dengan menggunakan fungsi initial_split() dari library rsample. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode stratified random sampling, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set. set.seed(417) splitted &lt;- initial_split(data = credit_data, prop = 0.8, strata = &quot;Status&quot;) splitted #&gt; &lt;3565/889/4454&gt; Imputation dilakukan dengan menggunakan Step Functions - Imputation dari library recipes yang didefinisikan dalam sebuah recipe. rec &lt;- recipe(Status ~ ., training(splitted)) %&gt;% # `step_meanimpute()` dapat diganti dengan Step Functions - Imputation lain step_meanimpute(Income, Assets, Debt) %&gt;% prep() Setelah mendefinisikan proses data preparation pada objek rec, selanjutnya proses tersebut diterapkan ke data train menggunakan fungsi juice() dan ke data test menggunakan fungsi bake() dari library recipes. # membuat data train dengan fungsi `juice()` train &lt;- juice(rec) # membuat data test dengan fungsi `bake()` test &lt;- bake(rec, testing(splitted)) colSums(is.na(train)) #&gt; Seniority Home Time Age Marital Records Job Expenses #&gt; 0 5 0 0 1 0 1 0 #&gt; Income Assets Debt Amount Price Status #&gt; 0 0 0 0 0 0 colSums(is.na(test)) #&gt; Seniority Home Time Age Marital Records Job Expenses #&gt; 0 1 0 0 0 0 1 0 #&gt; Income Assets Debt Amount Price Status #&gt; 0 0 0 0 0 0 Dari output di atas diketahui bahwa variabel Income, Assets, dan Debt sudah tidak mengandung missing value. Link artikel mengenai missing value 6 Different Ways to Compensate for Missing Values In a Dataset (Data Imputation with examples) Link mengenai Step Functions - Imputation dari library recipes Step Functions - Imputation Bab 4 Unsupervised Learning # libraries library(tidyverse) library(FactoMineR) library(factoextra) library(rsample) library(recipes) library(plotly) Penerapan PCA di industri? PCA pada industri lebih sering digunakan untuk data preparation sama halnya seperti scaling, feature engineering, ataupun variable selection. PCA digunakan untuk mereduksi data besar menjadi data yang lebih kecil, secara sederhana dapat dikatakan mengurangi jumlah kolom pada data. Walaupun PCA mengurangi jumlah kolom pada data (mereduksi dimensi), PCA tetap mempertahankan semua variabel (menggunakan semua variabel). Sebelum mereduksi dimensi PCA akan merangkum terlebih dahulu semua informasi yang terdapat pada setiap variabel ke dalam bentuk PC, PC tersebut yang nantinya akan direduksi (dikurangi) dimensinya. Oleh karena itu, variabel yang digunakan jumlahnya tetap sama seperti data awal, hanya informasi (variansinya) saja yang berkurang. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi An Application of PCA. Contoh permasalahan yang sering ditemui adalah klasifikasi dokumen. Saat ini semua administrasi dilakukan secara online/elektronik (tidak manual), adakalanya seorang nasabah/pelamar/customer harus melakukan upload dokumen. Sebelum adanya klasifikasi dokumen, pengecekkan kebenaran dokumen dilakukan secara manual. sehingga membutuhkan waktu yang cukup lama dan kapasitas penyimpanan yang relatif besar karena apps tidak mampu memilah mana dokumen yang sudah sesuai dan mana yang belum. Namun, permasalahan tersebut sudah mampu terjawab dengan adanya klasifikasi dokumen. Data untuk klasifikasi dokumen adalah data image yang jika dilakukan proses klasifikasi akan memerlukan komputasi yang relatif lama dibandingkan data tabular biasa. Oleh karena itu, perlu dilakukan PCA untuk mereduksi dimensi data image tersebut supaya komputasi saat proses klasifikasi bisa menjadi lebih cepat. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi Image Compression with PCA in R. Apakah biplot dapat menampilkan PC lain selain PC1 dan PC2? Bisa tetapi informasi yang dijelaskan menjadi berkurang, karena secara default pada R PC1 dan PC2 merangkum informasi paling banyak. Berikut contoh membuat biplot dengan menggunakan PC lain (selain PC1 dan PC2): head(USArrests) #&gt; Murder Assault UrbanPop Rape #&gt; Alabama 13.2 236 58 21.2 #&gt; Alaska 10.0 263 48 44.5 #&gt; Arizona 8.1 294 80 31.0 #&gt; Arkansas 8.8 190 50 19.5 #&gt; California 9.0 276 91 40.6 #&gt; Colorado 7.9 204 78 38.7 Membuat PCA dari data USArrests dengan menggunakan fungsi prcomp(). pca_us &lt;- prcomp(USArrests, scale = T) Membuat visualisasi dari hasil PCA dengan menggunakan fungsi biplot(). # parameter `choices` dapat diganti sesuai PC yang ingin dibuat, secara default menggunakan PC1 dan PC2 (choices = 1:2) biplot(pca_us, choices = 2:3) Dimensionality reduction mengatasi masalah high-dimensional data, permasalahan apa yang terdapat pada data berdimensi tinggi? menyulitkan pengolahan data, memerlukan komputasi yang besar, tidak efisien secara waktu. Perbedaan membuat PCA dengan menggunakan fungsi prcomp() dan PCA() dari library FactoMiner? Fungsi untuk membuat biplot di R: biplot(prcomp()) -&gt; base R plot.PCA(PCA()) -&gt; package FactoMineR kelebihan ketika membuat PCA dengan menggunakan fungsi PCA() dari library FactoMiner adalah bisa membuat biplot lebih spesifik (memisah dua grafik yang diajdikan satu -&gt; individu/variabel) dan bisa mengkombinasikan antara variabel numerik dan kategorik dengan menggunakan fungsi plot.PCA(). Apakah terdapat best practice dalam menentukan jumlah PC yang digunakan pada PCA? Penentuan jumlah PC yang digunakan bergantung pada kebutuhan analisa yang dilakukan. Namun, kembali pada tujuan awal melakukan PCA, yaitu untuk mereduksi dimensi supaya analisis lanjutan yang dilakukan memiliki waktu yang relatif cepat dan ruang penyimpanan yang lebih efisien. Sehingga, seringkali seorang analis menentapkan threshold lebih dari 70-75% informasi. Maksudnya jumlah PC yang digunakan adalah jumlah PC yang sudah merangkum kurang lebih 70-75% informasi. Namun, threshold tersebut sifatnya tidak mutlak artinya disesuaikan dengan kebutuhan analisis dan bisnis. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi How many components can I retrieve in principal component analysis?. Bagaimana best practice dalam penentuan jumlah cluster? Fungsi kmeans() tidak dapat menentukan jumlah cluster secara otomatis. Jumlah cluster tetap ditentukan oleh user berdasarkan kebutuhan bisnis. Namun, secara statistik penentuan jumlah cluster dapat dilakukan berdasarkan penurunan wss. Secara sederhana, penurunan wss dapat divisualisasikan dengan menggunakan fungsi fviz_nbclust() dari library factoextra. Berikut contoh memvisualisasikan penurunan wss dengan menggunakan data USArrests: head(USArrests, 6) #&gt; Murder Assault UrbanPop Rape #&gt; Alabama 13.2 236 58 21.2 #&gt; Alaska 10.0 263 48 44.5 #&gt; Arizona 8.1 294 80 31.0 #&gt; Arkansas 8.8 190 50 19.5 #&gt; California 9.0 276 91 40.6 #&gt; Colorado 7.9 204 78 38.7 # scaling data USArrests_scale &lt;- scale(USArrests) Melakukan visualisasi penurunan wss dengan menggunakan fungsi fviz_nbclust() dari library factoextra. set.seed(100) fviz_nbclust(USArrests_scale, method = &quot;wss&quot;, kmeans) Jumlah cluster yang dipilih adalah jumlah cluster yang ketika dilakukan penambahan cluster sudah tidak mengakibatkan penurunan wss yang signifikan (pada grafik bentuknya landai), kemudian disesuaikan dengan kebutuhan bisnis pada industri. Apakah kita dapat memvisualisasikan biplot dengan 3 dimensi? Untuk menampilkan biplot dengan 3 dimensi dapat menggunakan function plot_ly() dari package plotly. Berikut ini akan dicontohkan memvisualisasikan biplot dari PC1, PC2, PC3 dan juga akan dibedakan setiap titik observasi dengan cluster nya. Sebelum masuk ke visualisasi, akan dicari terlebih dahulu cluster untuk setiap observasi. # Read data in whiskies &lt;- read.csv(&quot;data/04-UL/whiskies.txt&quot;) # Distillery column is the name of each whisky rownames(whiskies) &lt;- whiskies[,&quot;Distillery&quot;] # remove RowID, Postcode, Latitude and Longitude whiskies &lt;- whiskies[,3:14] whi_km &lt;- kmeans(scale(whiskies), 4) Setelah menggunakan kmeans() untuk mendapatkan cluster, berikutnya kita lakukan PCA dan membentuk PC yang diperoleh dalam bentuk data frame. whis.pca&lt;-PCA(whiskies, graph = F,scale.unit = T) df_pca &lt;- data.frame(whis.pca$ind$coord) %&gt;% bind_cols(cluster = as.factor(whi_km$cluster)) head(df_pca) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 cluster #&gt; 1 -0.65565655 1.2056463 -0.1663438 -0.7807432 0.14526590 2 #&gt; 2 -2.31263102 3.7479878 1.3669186 0.8719922 0.69366566 2 #&gt; 3 -1.60215288 -0.6640822 -0.2972053 -1.1027897 -0.01535638 4 #&gt; 4 5.41363278 0.2448746 1.2101422 -0.7483052 -0.19536723 3 #&gt; 5 0.12164922 0.4127927 -0.3044621 -1.2705758 1.49597271 1 #&gt; 6 0.09941062 -1.3966133 -1.2024542 1.6549138 -0.28659985 4 Langkah berikutnya adalah memvisualisasikan PC dan membedakan warna observasi berdasarkan cluster nya. plot_ly(df_pca,x = ~Dim.1, y = ~Dim.2, z = ~Dim.3, color = ~cluster) Bagaimana implementasi PCA pada data pre-processing? Berikut ini aku dilakukan implementasi PCA pada tahapan data pre-processing dengan menggunakan data attrition. attrition &lt;- read.csv(&quot;data/04-UL/attrition.csv&quot;) head(attrition, 5) #&gt; attrition age business_travel daily_rate department #&gt; 1 yes 41 travel_rarely 1102 sales #&gt; 2 no 49 travel_frequently 279 research_development #&gt; 3 yes 37 travel_rarely 1373 research_development #&gt; 4 no 33 travel_frequently 1392 research_development #&gt; 5 no 27 travel_rarely 591 research_development #&gt; distance_from_home education education_field employee_count employee_number #&gt; 1 1 2 life_sciences 1 1 #&gt; 2 8 1 life_sciences 1 2 #&gt; 3 2 2 other 1 4 #&gt; 4 3 4 life_sciences 1 5 #&gt; 5 2 1 medical 1 7 #&gt; environment_satisfaction gender hourly_rate job_involvement job_level #&gt; 1 2 female 94 3 2 #&gt; 2 3 male 61 2 2 #&gt; 3 4 male 92 2 1 #&gt; 4 4 female 56 3 1 #&gt; 5 1 male 40 3 1 #&gt; job_role job_satisfaction marital_status monthly_income #&gt; 1 sales_executive 4 single 5993 #&gt; 2 research_scientist 2 married 5130 #&gt; 3 laboratory_technician 3 single 2090 #&gt; 4 research_scientist 3 married 2909 #&gt; 5 laboratory_technician 2 married 3468 #&gt; monthly_rate num_companies_worked over_18 over_time percent_salary_hike #&gt; 1 19479 8 y yes 11 #&gt; 2 24907 1 y no 23 #&gt; 3 2396 6 y yes 15 #&gt; 4 23159 1 y yes 11 #&gt; 5 16632 9 y no 12 #&gt; performance_rating relationship_satisfaction standard_hours #&gt; 1 3 1 80 #&gt; 2 4 4 80 #&gt; 3 3 2 80 #&gt; 4 3 3 80 #&gt; 5 3 4 80 #&gt; stock_option_level total_working_years training_times_last_year #&gt; 1 0 8 0 #&gt; 2 1 10 3 #&gt; 3 0 7 3 #&gt; 4 0 8 3 #&gt; 5 1 6 3 #&gt; work_life_balance years_at_company years_in_current_role #&gt; 1 1 6 4 #&gt; 2 3 10 7 #&gt; 3 3 0 0 #&gt; 4 3 8 7 #&gt; 5 3 2 2 #&gt; years_since_last_promotion years_with_curr_manager #&gt; 1 0 5 #&gt; 2 1 7 #&gt; 3 0 0 #&gt; 4 3 0 #&gt; 5 2 2 Sebelum melakukan PCA terlebih dahulu dilakukan cross validation, yaitu membagi data menjadi training set untuk proses pemodelan dan testing set untuk melakukan evaluasi. Namun, data train dan data test tidak langsung dimasukkan ke dalam sebuah objek melainkan dilakukan PCA terlebih dahulu. Cross validation akan dilakukan dengan menggunakan fungsi initial_split() dari library rsample. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode stratified random sampling, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set. set.seed(417) splitted &lt;- initial_split(data = attrition, prop = 0.8, strata = &quot;attrition&quot;) splitted #&gt; &lt;1177/293/1470&gt; Melakukan tahapan data preparation yang didalamnya termasuk melakukan PCA. Data preparation yang akan dilakukan adalah menghapus variabel yang dianggap tidak berpengaruh, membuang variabel yang variansinya mendekati 0 (tidak informatif), melakukan scaling, dan melakukan PCA. Proses yang dilakukan pada tahapan data preparation akan dilakukan dengan menggunakan fungsi dari library recipes, yaitu step_rm() untuk menghapus variabel, step_nzv() untuk membuang variabel yang variansinya mendekati 0,step_center() dan step_scale() untuk melakukan scaling, terakhir step_pca() untuk melakukan PCA. rec &lt;- recipe(attrition ~ ., training(splitted)) %&gt;% step_rm(employee_count, employee_number) %&gt;% step_nzv(all_predictors()) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_pca(all_numeric(), threshold = 0.8) %&gt;% prep() Setelah mendefinisikan proses data preparation pada objek rec, selanjutnya proses tersebut diterapkan ke data train menggunakan fungsi juice() dan ke data test menggunakan fungsi bake() dari library recipes. train &lt;- juice(rec) # melihat hasil data train setelah dilakukan tahapan data preparation head(train, 5) #&gt; # A tibble: 5 x 21 #&gt; business_travel department education_field gender job_role marital_status #&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 travel_rarely sales life_sciences female sales_e… single #&gt; 2 travel_frequen… research_… life_sciences male researc… married #&gt; 3 travel_rarely research_… other male laborat… single #&gt; 4 travel_frequen… research_… life_sciences female researc… married #&gt; 5 travel_rarely research_… medical male laborat… married #&gt; # … with 15 more variables: over_time &lt;fct&gt;, attrition &lt;fct&gt;, PC01 &lt;dbl&gt;, #&gt; # PC02 &lt;dbl&gt;, PC03 &lt;dbl&gt;, PC04 &lt;dbl&gt;, PC05 &lt;dbl&gt;, PC06 &lt;dbl&gt;, PC07 &lt;dbl&gt;, #&gt; # PC08 &lt;dbl&gt;, PC09 &lt;dbl&gt;, PC10 &lt;dbl&gt;, PC11 &lt;dbl&gt;, PC12 &lt;dbl&gt;, PC13 &lt;dbl&gt; test &lt;- bake(rec, testing(splitted)) # melihat hasil data test setelah dilakukan tahapan data preparation head(test, 5) #&gt; # A tibble: 5 x 21 #&gt; business_travel department education_field gender job_role marital_status #&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 travel_rarely research_… medical female laborat… married #&gt; 2 travel_rarely research_… life_sciences male laborat… divorced #&gt; 3 travel_rarely research_… medical male healthc… married #&gt; 4 travel_rarely research_… medical male laborat… divorced #&gt; 5 travel_rarely research_… life_sciences male laborat… single #&gt; # … with 15 more variables: over_time &lt;fct&gt;, attrition &lt;fct&gt;, PC01 &lt;dbl&gt;, #&gt; # PC02 &lt;dbl&gt;, PC03 &lt;dbl&gt;, PC04 &lt;dbl&gt;, PC05 &lt;dbl&gt;, PC06 &lt;dbl&gt;, PC07 &lt;dbl&gt;, #&gt; # PC08 &lt;dbl&gt;, PC09 &lt;dbl&gt;, PC10 &lt;dbl&gt;, PC11 &lt;dbl&gt;, PC12 &lt;dbl&gt;, PC13 &lt;dbl&gt; Dari output di atas diketahui bahwa variabel numerik sudah berbentuk sebuah PC. Bagaimana penggunaan visualisasi PCA menggunakan package factoextra? 4.1 PCA Kita akan mencoba melakukan visualisasi pada data setelah dilakukan PCA dengan menggunakan data loan. loan &lt;- read.csv(&quot;data/04-UL/loan2017Q4.csv&quot;) head(loan, 5) #&gt; initial_list_status purpose int_rate installment annual_inc dti #&gt; 1 w debt_consolidation 14.08 675.99 156700 19.11 #&gt; 2 f debt_consolidation 9.44 480.08 50000 19.35 #&gt; 3 w debt_consolidation 28.72 1010.30 25000 65.58 #&gt; 4 w debt_consolidation 13.59 484.19 175000 12.60 #&gt; 5 w major_purchase 15.05 476.33 109992 10.00 #&gt; verification_status grade revol_bal inq_last_12m delinq_2yrs home_ownership #&gt; 1 Source Verified C 21936 3 0 MORTGAGE #&gt; 2 Not Verified B 5457 1 1 RENT #&gt; 3 Verified F 23453 0 0 OWN #&gt; 4 Not Verified C 31740 0 0 MORTGAGE #&gt; 5 Not Verified C 2284 3 0 MORTGAGE #&gt; not_paid log_inc verified grdCtoA #&gt; 1 0 11.96209 1 0 #&gt; 2 1 10.81978 0 1 #&gt; 3 1 10.12663 1 0 #&gt; 4 1 12.07254 0 0 #&gt; 5 0 11.60816 0 0 Sebelum melakukan PCA kita akan melakukan tahapan data preparation terlebih dahulu dengan membuang variabel initial_list_status, home_ownership, dan not_paid karena visualisasi yang akan dibuat tidak memerlukan insight dari ketiga variabel tersebut. loan_clean &lt;- loan %&gt;% select(-c(initial_list_status, home_ownership, not_paid)) Membuat PCA dengan menggunakan fungsi PCA() dari library FactoMiner. Parameter yang digunakan adalah: ncp: Jumlah PC yang akan dihasilkan. Secara default fungsi PCA() hanya akan menampilkan 5 PC awal (5 PC yang membawa informasi paling banyak). quali.sup: Nomor kolom dari variabel kategorik. graph: Sebuah logical value. T akan menampilkan hasil visualisasi, F tidak menampilkan hasil visualisasi. Secara default fungsi PCA() akan langsung menampilkan hasil visualisasi. pca_loan &lt;- PCA(loan_clean, ncp = 10, quali.sup = c(1, 6, 7), graph = F) Setelah membuat PCA, selanjutnaya adalah membuat visualisasi dari hasil PCA. Kita akan membuat individual plot menggunakan fungsi fviz_pca_ind() dari library factoextra. Parameter yang digunakan adalah: Hasil PCA habillage: Variabel kategori yang dipilih, setiap individu akan dibedakan berdasarkan variabel kategori yang dipilih. select.ind: Jumlah individu dengan kontribusi tertinggi yang ingin dilihat. fviz_pca_ind(pca_loan, habillage = 6, select.ind = list(contrib = 10)) Plot individu di atas hanya menampilkan 10 observasi yang memberikan informasi tertinggi terhadap PC1 dan PC2. Namun, terdapat lebih dari 10 titik observasi yang terdapat pada plot di atas karena terdapat titik observasi yang merupakan titik pusat dari tiap status verifikasi. 4.2 Cluster Kita akan mencoba melakukan visualisasi hasil clustering dengan menggunakan data USArrests. head(USArrests, 5) #&gt; Murder Assault UrbanPop Rape #&gt; Alabama 13.2 236 58 21.2 #&gt; Alaska 10.0 263 48 44.5 #&gt; Arizona 8.1 294 80 31.0 #&gt; Arkansas 8.8 190 50 19.5 #&gt; California 9.0 276 91 40.6 # scaling data `USArrests` USArrests_scale &lt;- scale(USArrests) Menentukan jumlah cluster yang akan dibuat berdasarkan penurunan wss. Supaya lebih mudah menentukan penurunan wss yang sudah tidak signifikan ketika menambah jumlah cluster, maka dibuat visualisasinya dengan menggunakan fungsi fviz_nbclust() dari library factoextra. set.seed(100) fviz_nbclust(USArrests_scale, method = &quot;wss&quot;, kmeans) Melakukan k-means clustering dengan jumlah cluster 5 berdasarkan hasil penurunan wss di atas menggunakan fungsi kmeans(). set.seed(100) USArrests_cl &lt;- kmeans(USArrests_scale, 5) Membuat visualisasi hasil cluster dengan menggunakan fungsi fviz_cluster() dari library factoextra. # `USArrests` bisa diganti dengan yang sudah dilakukan scaling `USArrests_scale` fviz_cluster(USArrests_cl, USArrests) Mengkombinasikan visualisasi hasil clusterig dengan PCA. Untuk melakukan hal tersebut kita harus menambahkan kolom cluster pada data USArrests. USArrests_clean &lt;- USArrests %&gt;% mutate(cluster = USArrests_cl$cluster) head(USArrests_clean, 5) #&gt; Murder Assault UrbanPop Rape cluster #&gt; 1 13.2 236 58 21.2 1 #&gt; 2 10.0 263 48 44.5 4 #&gt; 3 8.1 294 80 31.0 4 #&gt; 4 8.8 190 50 19.5 5 #&gt; 5 9.0 276 91 40.6 4 Kita akan mengubah nama baris yang awalnya berupa indeks menjadi nama negara sesuai dengan data USArrests. rownames(USArrests_clean) &lt;- rownames(USArrests) head(USArrests_clean, 5) #&gt; Murder Assault UrbanPop Rape cluster #&gt; Alabama 13.2 236 58 21.2 1 #&gt; Alaska 10.0 263 48 44.5 4 #&gt; Arizona 8.1 294 80 31.0 4 #&gt; Arkansas 8.8 190 50 19.5 5 #&gt; California 9.0 276 91 40.6 4 Membuat PCA terlebih dahulu untuk mengkombinasikan visualisasi hasil clustering dengan PCA dengan menggunakan PCA(). pca_USArrest &lt;- PCA(USArrests_clean, quali.sup = 5, graph = F) Mengkombinasikan visualisasi hasil clustering dan PCA menggunakan fungsi fviz_pca_biplot() dari library factoextra. Parameter yang digunakan adalah: Hasil PCA habillage: Variabel kategori yang dipilih, setiap individu akan dibedakan berdasarkan variabel kategori yang dipilih. addEllipses: Sebuah logical value. T akan menambah elips untuk ssetiap cluster, F sebaliknya. Secara default fungsi fviz_pca_biplot() tidak akan menambah elips pada plot individu. fviz_pca_biplot(pca_USArrest, habillage = 5, addEllipses = T) Dari plot di atas terlihat bahwa antar cluster saling tumpang tindih, namun kenyataannya antar cluster pasti memiliki observasi/individu yang unik. Hal tersebut terjadi karena kita mencoba untuk memvisualisasikan cluster yang dibentuk dari 4 variabel/dimensi mejadi 2 variabel/dimensi saja. Bab 5 Time Series and Forecasting Apa itu Time Series? Time series merupakan data yang diperoleh dan disusun berdasarkan urutan waktu. Waktu yang digunakan dapat berupa hari, minggu, bulan, dan sebagainya. Apa itu Seasonal Effects? Seasonal effects terjadi jika data observasi memiliki pola yang berulang sesuai dengan siklus tertentu Perbedaan times series dan regression? Time series adalah analis variabel numerik berdasarkan deret waktu. Perbedaan mendasar dengan regresi, jika regresi memprediksi berdasarkan variabel independen lainnya (x1, x2, x3). Sedangkan untuk time series kita hanya mengamati variabel y yang akan kita prediksi Apa arti dari nilai smoothing parameter yang mendekati nilai 1? Nilai smoothing paremeter mendekati nilai 1 artinya bobot lebih besar diberikan ke data observasi terbaru Pada time series forecasting, data deret waktu yang dimiliki harus lengkap tanpa ada tanggal yang hilang, bagaimana mengatasi data yang tanggal nya tidak lengkap? library(lubridate) library(dplyr) Quantity &lt;- c(3,4,5) Order.Date &lt;- c(&quot;2019-01-03&quot;,&quot;2019-01-07&quot;,&quot;2019-01-08&quot;) dat &lt;- data.frame(Order.Date, Quantity) %&gt;% mutate(Order.Date = ymd(Order.Date)) Gunakan function pad() dari package padr untuk memenuhi tanggal yang hilang library(padr) dat %&gt;% pad() #&gt; Order.Date Quantity #&gt; 1 2019-01-03 3 #&gt; 2 2019-01-04 NA #&gt; 3 2019-01-05 NA #&gt; 4 2019-01-06 NA #&gt; 5 2019-01-07 4 #&gt; 6 2019-01-08 5 Bagaimana mengisi nilai NA pada time series object? Fill = \"extend\" adalah salah satu function untuk mengisi nilai NA dengan nilai disekitarnya. link library(zoo) dat %&gt;% pad() %&gt;% pull(Quantity) %&gt;% ts(frequency = 1) %&gt;% na.fill(fill = &quot;extend&quot;) #&gt; Time Series: #&gt; Start = 1 #&gt; End = 6 #&gt; Frequency = 1 #&gt; [1] 3.00 3.25 3.50 3.75 4.00 5.00 Jika hasil decomposition pada trend masih membentuk pola berulang, apa yang terjadi? Ketika hasil decomposition yang diperoleh pada trend masih membentuk pola berulang, itu artinya masih terdapat pola seasonal yang belum tertangkap, kemungkinan data yang digunakan memiliki multiple seasonal, untuk membuat object ts pada pola data multiple seasonal dapat menggunakan function msts(). Berikut merupakan link eksternal untuk penanganan Multiple Seasonal. Untuk penanganan multiple seasonal lebih lengkap nya dapat di cek pada link berikut ini. multiple seasonal Apakah pada metode arima kita dapat menambahkan variable prediktor pada analisis? Untuk analisis time series dengan variabel prediktor lainnya dapat menggunakan parameter xreg pada function Arima() dan auto.arima(). library(fpp2) library(forecast) Arima(y = uschange[,1], xreg = uschange[,2], order = c(1,1,0)) #&gt; Series: uschange[, 1] #&gt; Regression with ARIMA(1,1,0) errors #&gt; #&gt; Coefficients: #&gt; ar1 xreg #&gt; -0.5412 0.1835 #&gt; s.e. 0.0638 0.0429 #&gt; #&gt; sigma^2 estimated as 0.3982: log likelihood=-177.46 #&gt; AIC=360.93 AICc=361.06 BIC=370.61 auto.arima(y = uschange[,1], xreg = uschange[,2]) #&gt; Series: uschange[, 1] #&gt; Regression with ARIMA(1,0,2) errors #&gt; #&gt; Coefficients: #&gt; ar1 ma1 ma2 intercept xreg #&gt; 0.6922 -0.5758 0.1984 0.5990 0.2028 #&gt; s.e. 0.1159 0.1301 0.0756 0.0884 0.0461 #&gt; #&gt; sigma^2 estimated as 0.3219: log likelihood=-156.95 #&gt; AIC=325.91 AICc=326.37 BIC=345.29 Untuk detail lengkapnya dapat di lihat pada link berikut ini: Regression ARIMA Nilai error yang harus dilihat dan diperhatikan? Kapan menggunakan MAE/RMSE/MAPE? Tidak ada jawaban pasti untuk mengetahui penggunaan ketiga error tersebut. Tentunya setiap indikator memiliki memiliki kelebihan dan kekurangan masing-masing. Berikut ringkasan dari ketiga error tersebut: MAE(Mean Absolute Error), hasil MAE tidak akan terpengaruh jika memiliki data outlier. RMSE (Root Mean Square Error), memberikan jaminan untuk mendapatkan hasil perkiraan yang tidak bias karena cara hitungnya dengan mengkuadratkan error yang diperoleh, namun ketika memiliki data outlier pada data tentunya RMSE memiliki kecenderungan untuk memperoleh perkiraan yang besar. MAPE (Mean Absolute Percentage Error), MAPE menunjukan rata-rata kesalahan absolut peramalan dalam bentuk presentase terhadap data aktual. MAPE tidak cocok jika memiliki observasi yang bernilai 0, karena cara hitung MAPE adalah dengan membagi dengan nilai aktual, hal tersebut akan menyebabkan nilai MAPE menjadi infinit. Apakah hasil diff manual berbeda dengan yang dilakukan pada fungsi arima atau auto.arima ? Hasil prediksi yang diperoleh ketika melakukan differencing manual kemudian diaplikasikan dengan function arima/auto.arima akan sedikit berbeda ketika langsung melakukan differencing dari function arima/auto.arima. Hal ini tentunya tidak menjadi masalah besar, karena hasil yang diperoleh tidak jauh berbeda. Untuk detail rumus yang digunakan dapat dilihat di link berikut. differencing Bab 6 Neural Network and Deep Learning library(rsample) library(recipes) library(keras) Berapa jumlah hidden layer dan nodes untuk setiap hidden layer secara best practice dalam membangun arsitektur neural network (ANN) ? Kebanyakan orang menggunakan minimal 2 hidden layer, namun tidak menutup kemungkinan menggunakan lebih dari 2 ataupun kurang dari 2 hidden layer. Jumlah nodes biasanya semakin mengecil ketika hidden layers semakin dekat dengan output layer. Tujuannya adalah untuk melihat fitur dengan lebih spesifik. Kebanyakan orang menggunakan angka biner \\(2^{n}\\) seperti 1, 2, 4, 8, 16, 32, 64, 128, 256, dst karena neural network merupakan metode yang berasal dari orang computer science dan mathematics yang biasa menggunakan angka biner. Fungsi aktivasi apa yang sering digunakan ketika membuat arsitektur neural network ? Pada hidden layer biasa digunakan fungsi aktivasi relu karena relu melakukan transformasi data dengan mengubah nilai negatif menjadi 0 dan membiarkan nilai positif, sehingga semakin ke belakang informasi yang dibawa tidak banyak berkurang. Pada output layer: jika casenya adalah regresi digunakan fungsi aktivasi linear, jika casenya adalah klasifikasi biner digunakan fungsi aktivasi sigmoid, dan jika casenya adalah klasifikasi multiclass digunakan fungsi aktivasi softmax. Bagaimana menentukan batch size dan jumlah epoch ? Batch size biasanya menggunakan angka yang dapat membagi habis jumlah data, supaya data yang tersedia dapat digunakan secara keseluruhan (tidak ada yang tidak terpakai). Contoh: Jika data train terdiri dari 800 observasi, kita bisa menggunakan batch size 200 yang dapat membagi habis 80 observasi. Jumlah epoch dimulai dari angka yang kecil terlebih dahulu supaya komputasi yang dilakukan tidak terlalu lama, kemudian dilihat apakah error dan accuracy yang dhasilkan sudah konvergen atau belum. Jika belum bisa menambahkan jumlah epoch sedikit demi sedikit, dan sebaliknya. Bagaimana menentukan learning rate yang tepat ? Learning rate berfungsi mempercepat atau memperlambat besaran update error. Semakin besar learning rate, maka error/accuracy akan semakin cepat konvergen. Namun, bisa saja titik error paling minimum (global optimum) terlewat. Semakin kecil learning rate, maka terdapat kemungkinan yang lebih besar untuk sampai di titik error paling minimum (global optimum). Namun, error/accuracy akan lebih lama konvergen. Optimizer apa yang paling sering digunakan ? Optimizer merupakan fungsi yang digunakan untuk mengoptimumkan error (memperkecil error). Secara sederhana untuk mengoptimumkan suatu fungsi bisa melalui fungsi turunan, pada neural network disebut sgd. Namun, sgd memiliki beberapa kekurangan sehingga mulai banyak yang memperbaiki fungsi sgd tersebut. Untuk sekarang ini salah satu optimizer yang cukup terkenal adalah adam sebagai optimizer yang merupakan perbaikan dari sgd karena optimizer tersebut dapat mengupdate/menyesuaikan momentum ketika proses optimisasi. Berikut link eksternal yang dapat dijadikan sebagai bahan referensi Adaptive Moment Estimation (Adam) Selain tips di atas berikut link eksternal yang dapat dijadikan referensi dalam membangun arsitektur neural network Rules-of-thumb for building a Neural Network Perbedaan metode-metode machine learning dengan neural network dan deep learning ? Neural network bukan merupakan metode yang berasal dari orang statistik melainkan lahir dari pemikiran orang-orang computer science dan math. Neural network merupakan salah satu metode machine learning, neural network yang arsitekturnya sudah cukup rumit sering disebut sebagai deep learning. Neural network memilki 1 hidden layer, sementara deep learning memiliki &gt; 1 hidden layer. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi Deep learning &amp; Machine learning: what’s the difference? Bagaimana mentransformasikan prediktor data kategorik menjadi variabel dummy? Kita akan menggunakan data attrition yang memiliki variabel kategorik untuk dilakukan dummy transformation sebelum menggunakan metode neural network. attrition &lt;- read.csv(&quot;data/06-NN/attrition.csv&quot;) head(attrition, 5) #&gt; attrition age business_travel daily_rate department #&gt; 1 yes 41 travel_rarely 1102 sales #&gt; 2 no 49 travel_frequently 279 research_development #&gt; 3 yes 37 travel_rarely 1373 research_development #&gt; 4 no 33 travel_frequently 1392 research_development #&gt; 5 no 27 travel_rarely 591 research_development #&gt; distance_from_home education education_field employee_count employee_number #&gt; 1 1 2 life_sciences 1 1 #&gt; 2 8 1 life_sciences 1 2 #&gt; 3 2 2 other 1 4 #&gt; 4 3 4 life_sciences 1 5 #&gt; 5 2 1 medical 1 7 #&gt; environment_satisfaction gender hourly_rate job_involvement job_level #&gt; 1 2 female 94 3 2 #&gt; 2 3 male 61 2 2 #&gt; 3 4 male 92 2 1 #&gt; 4 4 female 56 3 1 #&gt; 5 1 male 40 3 1 #&gt; job_role job_satisfaction marital_status monthly_income #&gt; 1 sales_executive 4 single 5993 #&gt; 2 research_scientist 2 married 5130 #&gt; 3 laboratory_technician 3 single 2090 #&gt; 4 research_scientist 3 married 2909 #&gt; 5 laboratory_technician 2 married 3468 #&gt; monthly_rate num_companies_worked over_18 over_time percent_salary_hike #&gt; 1 19479 8 y yes 11 #&gt; 2 24907 1 y no 23 #&gt; 3 2396 6 y yes 15 #&gt; 4 23159 1 y yes 11 #&gt; 5 16632 9 y no 12 #&gt; performance_rating relationship_satisfaction standard_hours #&gt; 1 3 1 80 #&gt; 2 4 4 80 #&gt; 3 3 2 80 #&gt; 4 3 3 80 #&gt; 5 3 4 80 #&gt; stock_option_level total_working_years training_times_last_year #&gt; 1 0 8 0 #&gt; 2 1 10 3 #&gt; 3 0 7 3 #&gt; 4 0 8 3 #&gt; 5 1 6 3 #&gt; work_life_balance years_at_company years_in_current_role #&gt; 1 1 6 4 #&gt; 2 3 10 7 #&gt; 3 3 0 0 #&gt; 4 3 8 7 #&gt; 5 3 2 2 #&gt; years_since_last_promotion years_with_curr_manager #&gt; 1 0 5 #&gt; 2 1 7 #&gt; 3 0 0 #&gt; 4 3 0 #&gt; 5 2 2 Kita akan melakukan cross validation, yaitu membagi data menjadi training set untuk proses pemodelan dan testing set untuk melakukan evaluasi. Namun, data train dan data test tidak langsung dimasukkan ke dalam sebuah objek melainkan dilakukan tahapan data preparation terlebih dahulu yang di dalamnya terdapat tahapan dummy transformation. Cross validation akan dilakukan dengan menggunakan fungsi initial_split() dari library rsample. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode stratified random sampling, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set. set.seed(100) splitted &lt;- initial_split(attrition, prop = 0.8, strata = &quot;attrition&quot;) splitted #&gt; &lt;1177/293/1470&gt; Melakukan tahapan data preparation yang didalamnya termasuk melakukan dummy ransformation. Data preparation yang akan dilakukan adalah menghapus variabel yang dianggap tidak berpengaruh, membuang variabel yang variansinya mendekati 0 (tidak informatif), melakukan scaling, dan melakukan dummy transformation. Proses yang dilakukan pada tahapan data preparation akan dilakukan dengan menggunakan fungsi dari library recipies, yaitu step_rm() untuk menghapus variabel, step_nzv() untuk membuang variabel yang variansinya mendekati 0,step_center() dan step_scale() untuk melakukan scaling, terakhir step_dummy() untuk dummy transformation. rec &lt;- recipe(attrition ~ ., data = training(splitted)) %&gt;% step_rm(employee_count, employee_number) %&gt;% step_nzv(all_predictors()) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_dummy(all_nominal(), -attrition, one_hot = FALSE) %&gt;% prep() Setelah mendefinisikan proses data preparation pada objek rec, selanjutnya proses tersebut diterapkan ke data train menggunakan fungsi juice() dan ke data test menggunakan fungsi bake() dari library recipes. data_train &lt;- juice(rec) data_test &lt;- bake(rec, testing(splitted)) head(data_train, 5) #&gt; # A tibble: 5 x 45 #&gt; age daily_rate distance_from_h… education environment_sat… hourly_rate #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4.41e-1 0.747 -1.02 -0.899 -0.633 1.37 #&gt; 2 8.41e-4 1.42 -0.895 -0.899 1.19 1.27 #&gt; 3 -1.10e+0 -0.528 -0.895 -1.88 -1.55 -1.28 #&gt; 4 -5.49e-1 0.505 -0.895 -0.899 1.19 0.633 #&gt; 5 2.42e+0 1.30 -0.772 0.0789 0.281 0.731 #&gt; # … with 39 more variables: job_involvement &lt;dbl&gt;, job_level &lt;dbl&gt;, #&gt; # job_satisfaction &lt;dbl&gt;, monthly_income &lt;dbl&gt;, monthly_rate &lt;dbl&gt;, #&gt; # num_companies_worked &lt;dbl&gt;, percent_salary_hike &lt;dbl&gt;, #&gt; # performance_rating &lt;dbl&gt;, relationship_satisfaction &lt;dbl&gt;, #&gt; # stock_option_level &lt;dbl&gt;, total_working_years &lt;dbl&gt;, #&gt; # training_times_last_year &lt;dbl&gt;, work_life_balance &lt;dbl&gt;, #&gt; # years_at_company &lt;dbl&gt;, years_in_current_role &lt;dbl&gt;, #&gt; # years_since_last_promotion &lt;dbl&gt;, years_with_curr_manager &lt;dbl&gt;, #&gt; # attrition &lt;fct&gt;, business_travel_travel_frequently &lt;dbl&gt;, #&gt; # business_travel_travel_rarely &lt;dbl&gt;, department_research_development &lt;dbl&gt;, #&gt; # department_sales &lt;dbl&gt;, education_field_life_sciences &lt;dbl&gt;, #&gt; # education_field_marketing &lt;dbl&gt;, education_field_medical &lt;dbl&gt;, #&gt; # education_field_other &lt;dbl&gt;, education_field_technical_degree &lt;dbl&gt;, #&gt; # gender_male &lt;dbl&gt;, job_role_human_resources &lt;dbl&gt;, #&gt; # job_role_laboratory_technician &lt;dbl&gt;, job_role_manager &lt;dbl&gt;, #&gt; # job_role_manufacturing_director &lt;dbl&gt;, job_role_research_director &lt;dbl&gt;, #&gt; # job_role_research_scientist &lt;dbl&gt;, job_role_sales_executive &lt;dbl&gt;, #&gt; # job_role_sales_representative &lt;dbl&gt;, marital_status_married &lt;dbl&gt;, #&gt; # marital_status_single &lt;dbl&gt;, over_time_yes &lt;dbl&gt; Setelah melakukan dummy transformation pada pediktor, data train dan test harus disesuaikan bentuknya untuk melalui proses building model dengan metode neural network. Target variabel yang bertipe kategorik akan dilakukan dummy transformation dengan menggunakan fungsi to_categorical() dari library keras, sementara semua prediktor akan diubah ke dalam bentuk matriks numerik. # menyiapkan data train data_train_y &lt;- to_categorical(as.numeric(data_train$attrition) - 1) data_train_x &lt;- data_train %&gt;% select(-attrition) %&gt;% data.matrix() # menyiapkan data test data_test_y &lt;- to_categorical(as.numeric(data_test$attrition) - 1) data_test_x &lt;- data_test %&gt;% select(attrition) %&gt;% data.matrix() # cek data train dan test dim(data_train_x) #&gt; [1] 1177 44 dim(data_train_y) #&gt; [1] 1177 2 Ketika running model NN weight/bobot diinisialisasi secara random sehingga menyebabkan hasil yang berbeda jika dilakukan berulang kali. Bagaiamana cara mengatur/menggunakan seed pada Neural Network? Metode neural network selalu menginisialisasi bobot/weight secara random di awal, sehingga ketika metode tersebut di running berulang kali akan memperoleh hasil yang berbeda. Untuk mengatasi hal tersebut kita dapat menggunakan seed (state random). Kita dapat menentukan seed dengan menggunakan fungsi use_session_with_seed() dari library keras. use_session_with_seed(seed) Selain menggunakan cara di atas kita juga dapat menggunakan seed dengan fungsi initializer_random_normal(). Berikut cara menggunakan seed dengan fungsi tersebut. # define seed set.seed(100) initializer &lt;- initializer_random_normal(seed = 100) # use the seed when building architecture model &lt;- keras_model_sequential() %&gt;% layer_dense(units = ..., activation = &quot;...&quot;, input_shape = c(...), kernel_initializer = initializer, bias_initializer = initializer) "]
]
