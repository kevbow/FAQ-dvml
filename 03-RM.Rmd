# Regression Model

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)

# scientific notation
options(scipen = 9999)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# libraries
library(tidyverse)
```

## FAQ

1. Bagaimana penanganan data kategorik pada model linear regression?

   Dengan menggunakan function `lm` pada R otomatis akan mengubah tipe data kategorik menjadi dummy variabel. Dummy variable berfungsi untuk mengkuantitatifkan variabel yang bersifat kualitatif (kategorik). Dummy variabel hanya mempunyai dua nilai yaitu 1 dan 0. Dummy memiliki nilai 1 untuk salah satu kategori dan nol untuk kategori yang lain. Jika terdapat sebanyak `k` kategori untuk  suatu prediktor maka akan ditransformasi menjadi `k-1` dummy.

2. Mengapa untuk asumsi normality yang harus berdistribusi normal adalah error/residual ?

   Jika residual berdistribusi normal, itu artinya residual cenderung berkumpul di titik sekitar 0, dapat dikatakan hasil prediksi tidak terlalu melenceng jauh dari data actual.

   Error yang tidak berdistribusi normal disebabkan oleh:

-  distribusi target variabel memang tidak normal
-  Model yang digunakan tidak cocok, misal hubungan antara prediktor dengan target tidak linier melainkan kudratik/eksponensial/dll walaupun target variabel memiliki distribusi normal. 
-  Selain itu, error harus berdistribusi normal terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval).
-  Terdapat banyak data outlier

3. Untuk prediktor kategorik, bagaimana jika terdapat salah satu level yang tidak signifikan (p value > alpha)? apakah prediktor tersebut masih dianggap signifikan mempengaruhi target?

   Ketika terdapat salah satu level pada variabel kategori yang tidak signifikan, salah satu penyebab yang sering ditemui karena jumlah observasi pada level tersebut sangat sedikit dibandingkan dengan level lainnya. Jika hal tersebut terjadi kita perlu mempertimbangkan `business question` dari model tersebut, tentunya ketika kita hanya membuang level yang tidak signifikan kemampuan model tidak dapat memprediksi pada salah satu level tersebut. Ketika kita memiliki kebutuhan untuk variabel tersebut dan salah satu level signifikan, kita dapat menganggap level lainnya juga signifikan.

4. Pada fungsi `lm` sudah otomatis melakukan transformasi data kategorik dengan level pertama yang dijadikan basis. Bagaimana jika dilakukan reorder level (mengubah urutan level), apakah akan mengubah hasil pemodelan?

   Hasil pemodelan tidak akan berubah, mengubah urutan level hanya akan mengubah basis yang digunakan. 

5. Bagaimana jika terdapat prediktor yang tidak signifikan, tetapi secara bisnis seharusnya prediktor tersebut berpengaruh terhadap target?

   Ketika variabel yang kita gunakan tidak signifikan secara statistik, namun secara bisnis berpengaruh terhadap target yang dimiliki, kita akan tetap mempertahankan variabel tersebut, banyak faktor yang menyebabkan variabel tersebut tidak signikan, bisa jadi karena data yang dimiliki tidak cukup banyak, banyak data oulier, atau ragam data yang hanya sedikit.

6. Mengapa perlu dilakukan pengecekkan asumsi pada metode regresi linier?

   Pengecekkan asumsi dilakukan terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval). Ketika semua asumsi terpenuhi model dapat dikatakan `BLUE` (Best Linear Unbiased Estimator).

7. Jika sudah dilakukan berbagai alternatif untuk pemenuhan asumsi, namun masih terdapat asumsi yang tidak terpenuhi apa yang harus dilakukan?

   Ketika sudah dilakukan berbagai alternatif untuk memenuhi asumsi namun asumsi masih tidak terpenuhi, itu artinya data yang kita miliki tidak cocok menggunakan regresi linear, dapat dicoba dengan model lain.

8. Bagaimana jika diperoleh nilai AIC negatif?

   AIC dapat bernilai negatif atupun positif yang disebabkan oleh nilai dari fungsi maksimum likelihood berikut: 

   `AIC = 2k âˆ’ 2ln(L)`, dimana `k` merupakan jumlah parameter (jumlah prediktor dan intersep) dan `L` merupakan nilai dari fungsi maksimum likelihood.

   Namun, pada pemilihan model nilai AIC yang dilihat adalah nilai AIC yang sudah diabsolutkan. Sehingga tanda negatif/positif pada hasil AIC tidak berpengaruh dalam proses pemilihan model. Model yang dipilih adalah model yang memiliki nilai abolut AIC terkecil, hal ini mengindikasikan bahwa semakin sedikit model tersebut kehilangan informasi yang dibawa. [Negative values for AIC](https://stats.stackexchange.com/questions/84076/negative-values-for-aic-in-general-mixed-model)

9. Perbedaan dari R-squared dan Adjusted R-squared?

    R-squared memperhitungkan variasi dari semua variabel independen terhadap variabel dependen. Sehingga setiap penambahan variabel independen akan meningkatkan nilai R-squared. Sedangkan pada adjusted r-squared akan memperhitungan variasi dari variabel independen yang signifikan terhadap variabel dependen. Oleh karena itu, pada multiple linear regression disarankan untuk melihat nilai Adjusted R-squared.

10. Apa itu outlier?

    Data observasi yang terlihat sangat berbeda jauh dari observasi-observasi lainnya dan muncul dalam bentuk nilai ekstrim baik untuk sebuah variabel tunggal atau kombinasi.

11. Cara yang dapat dilakukan untuk tuning model regresi?

    Banyak cara yang dapat dilakukan untuk tuning model regresi, salah satunya adalah dengan deteksi outlier pada data observasi. Deteksi outlier dari data yang dimiliki, apakah dengan atau tanpa data outlier tersebut akan mengganggu perfomance model yang dimiliki.

12. Untuk apa ada p-value di output regresi jika sebelumnya kita sudah melakukan uji korelasi?

    Uji korelasi pada preprocessing data dilakukan untuk melihat secara umum apakah variabel prediktor dan target terdapat hubungan kuat atau tidak. Sedangkan uji pvalue pada output regresi pada setiap variabel prediktor menyatakan apakah setiap variabel prediktor benar-benar mempengaruhi target secara statistik atau tidak.

13. Uji statistik apa yang dapat digunakan untuk uji normalitas dengan lebih dari 5000 observasi?

Uji normalitas dengan observasi yang lebih dari 5000 dapat menggunakan uji Kolmogorov Smirnov dengan code sebagai berikut:

```{r, eval=FALSE}
ks.test(model$residuals, "pnorm", mean = mean(model$residuals), sd = sd(model$residuals))
```

14. Bagaimana jika target variabel yang dimiliki berupa bilangan diskrit, apakah bisa dilakukan analisis regresi?

    Untuk analisis regresi dengan target variabel berupa bilangan diskrit dapat menggunakan regresi poisson. Untuk detail lengkapnya dapat dilihat di link berikut: [Regresi Poisson](https://algotech.netlify.com/blog/poisson-regression-and-neg-ative-binomial-regression/)

15. Bagaimana jika ingin melakukan `linearity test` sekaligus untuk setiap predictor terhadap `target variable`?

Untuk melakukan linearity test, kita dapat menggunakan function `cor.test`
```{r}
copiers <- read.csv("data/03-RM/copiers.csv") %>% select(-Row.ID)
cor.test(copiers$Sales, copiers$Profit)
```
Dengan Pearson's hypothesis test sebagai berikut:

H0: Korelasi tidak signifikan

H1: Korelasi signifikan

Sedangkan untuk melakukan linearity test sekaligus untuk setiap predictor dapat membuat function sebagai berikut:

```{r}
cor.test.all <- function(data,target) {
  names <- names(data %>% select_if(is.numeric))
  df <- NULL
  for (i in 1:length(names)) {
    y <- target
    x <- names[[i]] 
    p_value <- cor.test(data[,y], data[,x])[3]
    temp <- data.frame(x = x,
                       y = y,
                       p_value = as.numeric(p_value))
    df  <- rbind(df,temp)
  }
  return(df)
}

cor.test.all(data = copiers,target ="Profit")
```

## Mathematics Concept

Untuk mengestimasi nilai koefisien (beta), pertama-tama coba ingat kembali beberapa konsep pada workshop "Practical Statistics". Variance merupakan nilai yang menggambarkan seberapa bervariasi/beragamnya suatu variabel bertipe numerik/angka. Semakin besar nilai variance maka semakin beragam nilai dalam satu variabel (heterogen), sedangkan semakin kecil nilai variance maka semakin sama/mirip setiap observasi pada satu variabel (homogen). Data yang observasinya bernilai sama, maka variance sama dengan 0.

Sementara covariance merupakan nilai yang menggambarkan hubungan (positif/negatif/tidak ada hubungan) antara dua variabel numerik. Namun covariance tidak dapat menggambarkan seberapa erat/kuat hubungan tersebut karena nilai covariance tidak memilki batasan yang mutlak (- inf, + inf).

Dalam notasi matematika, anggap kita memiliki data yang terdiri dari 2 variabel, yaitu, $({X_i}, {Y_i})$, maka secara empiris nlai covariance diperoleh dari:  

$$Cov(X,Y) = \frac{1}{n-1}\sum\limits_{i=1}^{n}({X_i}-\bar{X})({Y_i}-\bar{Y})$$    

bisa juga diperoleh dari,

$$Cov(X,Y) = \frac{1}{n-1}(\sum\limits_{i=1}^{n}{X_i}{Y_i} - n\bar{X}\bar{Y})$$  

Jika formula dari covariance cukup rumit, coba ingat kembali formula dari variance:

$$S^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n}(X_i - \bar{X})^2$$  

pahami bahwa perbedaan variance dan covariance adalah variance hanya mengacu pada 1 variabel, sedangkan covariance mengacu pada 2 variabel. Maka, formula dari covariance:  

$$Cov(X,Y) = \frac{1}{n-1}\sum\limits_{i=1}^{n}({X_i}-\bar{X})({Y_i}-\bar{Y})$$  

Seperti yang telah dijelaskan di atas bahwa covariance menjelaskan jenis hubungan antara 2 variabel numerik. Namun, kita tidak dapat menilai seberapa erat/kuat hubungan antara keduanya karena interval nilai covariance yang tidak memiliki batasan. Oleh karena itu, kita bisa melakukan standarization terlebih dahulu terhadap 2 variabel numerik tersebut yang mengacu pada definisi correlation:

$$Cor(X, Y) = \frac{Cov(X,Y)}{{S_x}{S_y}}$$

beberapa fakta mengenai correlation:

- Cor(X,Y) = Cor(Y,X)  
- -1 <= Cor(X,Y) <= 1  
- Nilai correlation mendekati 1 artinya kedua variabel berhubungan erat dan hubungannya linier positif  
- Nilai correlation mendekati -1 artinya kedua variabel berhubungan erat dan hubungannya linier negatif
- Nilai correlation mendekati 0 artinya kedua variabel tidak saling berhubungan secara linier  

Untuk menggambarkan persebaran observasi antara x dan y, dapat dilakukan dengan menarik suatu garis lurus yang menggambarkan keseluruhan persebaran data. Dimana, untuk menarik suatu garis lurus diperlukan titik awal ($b0$) dan kemiringan garis ($b1$).

Lalu bagaimana cara mengestimasi $b0$ dan $b1$ yang optimal (dimana garis linier dapat menggambarkan keseluruhan persebaran data). Kita bisa menggunakan konsep kuadrat terkecil, untuk menemukan kombinasi $b0$ dan $b1$ yang meminimumkan jarak kuadrat antara titik pengamatan dengan garis linier:

$$\sum\limits_{i=1}^{n}\{{Y_i} - (\beta_0 + {\beta_1}{X_i} )\}^2$$

- Estimasi slope: 

$$\hat{\beta}_1 = Cor(Y,X)\frac{Sd(Y)}{Sd(X)}$$  

- Estimasi intercept: 

$$\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$$

**R-square** secara definisi adalah persentase total keragaman suatu target variabel yang dapat dijelaskan oleh prediktor variabel (model), dengan formula:  

$$R^2 = 1 - \frac{\sum\limits_{i=1}^n (\hat{Y_i} - \bar{Y})^2}{\sum\limits_{i=1}^n(Y_i - \bar{Y})^2}$$  

Beberapa fakta tentang R-square ($R^2$):


- $R^2$ adalah persentase total keragaman suatu target variabel yang dapat dijelaskan oleh model regresi  
- $0 \leq R^2 \leq 1$  

Salah satu alat statistik yang dapat digunakan untuk mengecek ada/tidak multicolinearity adalah **Variance Inflation Factor** (VIF). VIF mengukur peningkatan estimasi koefisien beta, jika antar prediktor saling berkorelasi. Secara matematis, nilai VIF diperoleh dengan meregresikan setiap prediktor dengan prediktor lain. Contoh: diketahui terdapat $X1, X2, ..., Xn$, nilai VIF untuk $X1$ diperoleh dari hasil regresi $X1$ dengan $X2, ..., Xn$, dst. Hasil regresi tersebut kemudian diterapkan pada formula berikut:

$$VIF = \frac{1}{1-R^2(x)}$$

Secara umum jika nilai VIF yang diperoleh lebih besar atau sama dengan 10, mengindikasikan terjadi multicolinearity (antar prediktor saling berkorelasi kuat).


