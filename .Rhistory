inv.logit(0.6931472)
curve(inv.logit(x), from = -10, to=10)
# read & inspect data
honors <- read.csv("data_input/sample.csv") %>%
select(-femalexmath)
str(honors)
# cek missing value
anyNA(honors)
# data wrangling
honors <- honors %>%
mutate(female = as.factor(female),
hon = as.factor(hon))
str(honors)
honors.logit <- glm(hon ~ 1, data= honors, family = "binomial")
summary(honors.logit)
# probability
table(honors$hon)
p <- 49/(151+49)
# odds
o <- p/(1-p)
# log of odds
log(o)
# log of odds
logit(p)
# log of odds -> odds
odds <- exp(-1.1255)
# odds -> probability
prob <- odds/(odds+1)
# log of odds -> probability
inv.logit(-1.1255)
odds
prob
honors.logit2 <- glm(hon ~ female, data=honors, family="binomial")
summary(honors.logit2)
table(honors$female)
p1 <- 109/(109+91)
logit(p1)
# proportion of female/male got honors
table(honors = honors$hon, female = honors$female)
# peluang female dapat honors
>>>>>>> 7e10dc79dd5972cd5f902db9c0686fbb6cc284a1
honf <- 32/(77+32)
# peluang male dapat honors
honm <- 17/(17+74)
# odds
oddsf <- honf/(1-honf)
oddsm <- honm/(1-honm)
# log of odds
log(oddsf/oddsm)
log(oddsm)
# odds female dapat honors
exp(0.5927822)
# log of odds
female <- -1.4709 + 0.5928 * 1
# log of odds -> peluang
inv.logit(female)
male <- -1.4709 + 0.5928 * 0
inv.logit(male)
honors.logit3 <- glm(hon ~ math, data = honors, family = "binomial")
summary(honors.logit3)
# log of odds
hon52 <- -9.79394 + 0.15634 * 52
hon53 <- -9.79394 + 0.15634 * 53
hon53-hon52
# log of odds -> odds
exp(0.15634)
exp(hon53)/exp(hon52)
# kalau naik 2 kali
hon54 <- -9.79394 + 0.15634 * 54
exp(hon52)*(1.169224^2)
exp(hon54)
honors.logit4 <- glm(hon ~ female + math , data = honors, family = "binomial")
summary(honors.logit4)
# female
exp(0.96531)
# math
exp(0.16422)
handoyo <- -10.80595 + (0.96531 * 0) + (0.16422 * 66)
inv.logit(handoyo)
nabiilah <- -10.80595 + (0.96531 * 1) + (0.16422 * 70)
inv.logit(nabiilah)
ifelse(0.5081418 > 0.5, "honors", "non-honors")
honors.logit$aic
honors.logit2$aic
honors.logit3$aic
honors.logit4$aic
honors.logit$null.deviance
honors.logit2$deviance
honors.logit3$deviance
honors.logit4$deviance
honors.logit5 <- glm(hon~female+read, family = "binomial", data = honors)
summary(honors.logit5)
#log of odds -> odds
exp(0.94474)
#log of odds -> odds
exp(0.12567)
lods_n <- -8.58001 + 0.94474*1 + 0.12567*69
lods_n
inv.logit(lods_n)
ifelse(0.2055811 < 0.5, "non-honors", "honors")
library(brglm2)
glm(hon ~ female + read + math + write, data=honors, family="binomial", method = "detect_separation")
honors.logit6 <- glm(hon~., data = honors, family = "binomial", maxit = 50)
summary(honors.logit6)
# log of odds -> odds
exp(48.13764)
table(honors$hon, honors$write)
plot(honors$hon, honors$write)
GGally::ggpairs(honors)
# revisi model
honors.logit7 <- glm(hon~female + read + math, data = honors, family = "binomial")
summary(honors.logit7)
flight <- read.csv("data_input/flight_sm.csv") %>%
mutate(DepDel15 = as.factor(DepDel15))
str(flight)
set.seed(416)
sample(6,1)
flight.model <- glm(DepDel15 ~ Month + DayofWeek, data = flight, family = "binomial")
summary(flight.model)
exp(flight.model$coefficients[2])
exp(flight.model$coefficients[3])
library(car)
vif(flight.model)
# penyesuaian
flight$Month <- as.factor(flight$Month)
flight$DayofWeek <- as.factor(flight$DayofWeek)
# pembuatan model baru
flight.model2 <- glm(DepDel15 ~ Month + DayofWeek, data = flight, family = "binomial")
summary(flight.model2)
plot(table(flight$Month, flight$DepDel15))
loans <- read.csv("data_input/loan2017Q4.csv")
glimpse(loans)
summary(loans)
# data wrangling
loans <- loans %>%
select(-log_inc, -verification_status, -grade) %>%
mutate(not_paid = as.factor(not_paid),
verified = as.factor(verified),
grdCtoA = as.factor(grdCtoA))
# cek missing value
colSums(is.na(loans))
# explore with summary
summary(loans)
# explore with hist
hist(loans$delinq_2yrs)
# numeric predictor vs target variable
plot(loans$not_paid, loans$dti)
# categorical predictor vs target variable
plot(table(loans$purpose, loans$not_paid))
table(loans$not_paid)
# splitting
set.seed(417)
intrain <- sample(nrow(loans), nrow(loans)*0.8)
loans.train <- loans[intrain, ]
loans.test <- loans[-intrain, ]
# re-check class imbalance
table(loans.train$not_paid)
creditrisk <- glm(not_paid ~ verified + purpose + installment + int_rate + home_ownership + grdCtoA + annual_inc, loans.train, family="binomial")
summary(creditrisk)
# small business
exp(0.843750698)
# int_rate
exp(0.012309962)
# logistic regression juga dapat menggunakan step
step(creditrisk, direction = "backward")
# save model hasil stepwise
creditrisk2 <- glm(formula = not_paid ~ verified + purpose + installment + grdCtoA +
annual_inc, family = "binomial", data = loans.train)
summary(creditrisk2)
predict(creditrisk2, newdata = loans.test[1:6,], type = "link")
predict(creditrisk2, newdata = loans.test[1:6,], type = "response")
loans.test$pred.Risk <- predict(creditrisk2, newdata = loans.test, type = "response")
# ifelse(kondisi, benar, salah)
loans.test$pred.Label <- ifelse(loans.test$pred.Risk > 0.5, "1", "0")
# ubah kelas target (aktual dan prediksi) menjadi factor
loans.test <- loans.test %>%
mutate(pred.Label = as.factor(pred.Label))
# lihat hasil prediksi
loans.test %>%
select(pred.Risk, pred.Label, not_paid)
# confusion matrix sederhana
table(predicted = loans.test$pred.Label,
actual = loans.test$not_paid)
library(caret)
confusionMatrix(data = loans.test$pred.Label,
reference = loans.test$not_paid,
positive = "1")
(94+87)/nrow(loans.test)
94/(94+57)
94/(94+74)
87/(87+74)
# ifelse(kondisi, benar, salah)
loans.test$pred.Label2 <- ifelse(loans.test$pred.Risk > 0.45, "1", "0")
# ubah kelas target (aktual dan prediksi) menjadi factor
loans.test <- loans.test %>%
mutate(pred.Label2 = as.factor(pred.Label2))
# confusion matrix
confusionMatrix(data = loans.test$pred.Label2,
reference = loans.test$not_paid,
positive = "1")
# read
wa_churn <- read.csv("watson-churn.csv")
# inspect
glimpse(wa_churn)
summary(wa_churn)
# ini datanya aku rapihin dikit tadi.
# wa_churn <- wa_churn %>%
#   filter(PhoneService != "No",
#          InternetService != "No") %>%
#   select(-PhoneService, -InternetService) %>%
#   droplevels()
#
# write.csv(wa_churn, "watson-churn.csv", row.names = F)
# data wrangling
wa_churn <- wa_churn %>%
select(-customerID) %>%
mutate(SeniorCitizen = as.factor(SeniorCitizen))
# check missing value
anyNA(wa_churn)
colSums(is.na(wa_churn))
nrow(wa_churn)
wa_churn <- na.omit(wa_churn)
# EDA: target proportion
prop.table(table(wa_churn$Churn))
# cross validation
set.seed(417)
idx <- sample(nrow(wa_churn), nrow(wa_churn)*0.8)
data_train <- wa_churn[idx,]
data_test <- wa_churn[-idx,]
# recheck proportion
prop.table(table(data_train$Churn))
churn_model <- glm(Churn ~ ., data = data_train, family = "binomial")
summary(churn_model)
step(churn_model, direction = "backward")
churn_step <- glm(formula = Churn ~ SeniorCitizen + tenure + OnlineSecurity +
OnlineBackup + DeviceProtection + TechSupport + StreamingMovies +
Contract + PaperlessBilling + PaymentMethod + MonthlyCharges +
TotalCharges, family = "binomial", data = data_train)
summary(churn_step)
# predict
data_test$pred.Risk <- predict(churn_step, newdata = data_test, type = "response")
# classify
data_test$pred.Label <- ifelse(data_test$pred.Risk > 0.5, "Yes", "No")
result <- data_test[,18:20] %>% mutate(pred.Label = as.factor(pred.Label))
# evaluation
confusionMatrix(data = result$pred.Label,
reference = result$Churn,
positive = "Yes")
# classify
data_test$pred.Label2 <- ifelse(data_test$pred.Risk > 0.43, "Yes", "No")
result2 <- data_test[,c(18:19,21)] %>% mutate(pred.Label2 = as.factor(pred.Label2))
# evaluation
confusionMatrix(data = result2$pred.Label2,
reference = result2$Churn,
positive = "Yes")
# Create our dataset
food <- data.frame(list(c("apple", "bacon", "banana", "carrot","celery", "cheese","cucumber", "fish", "grape", "green bean", "lettuce", "nuts", "pear", "shrimp","orange"), c(10,1,10,6,3,1,2,3,10,3,1,3,10,2,9), c(9,4,1,10,10,1,8,2,5,7,10,5,7,2,3), c("fruit", "protein", "fruit", "vegetable", "vegetable", "protein", "vegetable", "protein", "fruit", "vegetable", "vegetable", "protein", "fruit","protein", "fruit")))
# Give each feature appropriate names
colnames(food)<- c("Ingredient", "Sweetness", "Crunchiness", "Type")
food
library(ggplot2)
plot.fruit <- ggplot(food, aes(x=food$Sweetness, y=food$Crunchiness))+geom_point(alpha = 0.05)+geom_label(aes(label=food$Ingredient),nudge_x=0.5, nudge_y=0, label.size=0.6)+labs(x="how sweet the food tastes", y="how crunchy the food is")
plot.fruit
library(grid)
grob = grobTree(textGrob("tomato", x=0.6, y=0.4, hjust=0, gp=gpar(col="darkorange", fontsize=14)))
plot.fruit + annotation_custom(grob)
sqrt((6-3)^2 + (4-7)^2)
# euclidean distance
sqrt((6-3)^2 + (4-7)^2)
# finding optimum k: square root dari jumlah data
nrow(food)
sqrt(nrow(food))
# jumlah k harus ganjil untuk mencegah kondisi seri pada majority voting
# importing the data and quickly explore the first 5 variables of the dataset
wbcd <- read.csv("data_input/wisc_bc_data.csv", stringsAsFactors = FALSE)
glimpse(wbcd)
summary(wbcd)
# data wrangling
wbcd <- wbcd %>%
select(-id)
# cross-validation
set.seed(417)
idx.knn <- sample(nrow(wbcd), 0.8*nrow(wbcd))
wbcd_train <- wbcd[idx.knn,]
wbcd_test <- wbcd[-idx.knn,]
train_x
# your code here
# scale train_x data
train_x <- scale(x = train_x)
# predictor variables in `train`
train_x <- train %>%
select_if(is.numeric) %>% data.frame()
turnover <- read.csv("turnover_balance.csv")
# your code here
turnover <- turnover %>%
mutate(Work_accident = as.factor(Work_accident),
left = as.factor(left),
promotion_last_5years = as.factor(promotion_last_5years))
set.seed(100)
# your code here
idx <- sample(nrow(turnover), nrow(turnover) * 0.8)
train <- turnover[idx,]
test <- turnover[-idx,]
model_logistic <- glm(left ~ ., data = train, family = "binomial")
# predictor variables in `train`
train_x <- train %>%
select_if(is.numeric) %>% data.frame()
# predictor variables in `test`
test_x <- test %>%
select_if(is.numeric) %>% data.frame()
# target variable in `train`
train_y <- train %>%
select(left)
# target variable in `test`
test_y <- test %>%
select(left)
# your code here
# scale train_x data
train_x <- scale(x = train_x)
# scale test_x data
test_x <- scale(x = test_x, center = attr(train_x, "scaled:center"), scale = attr(train_x, "scaled:scale"))
# train_x <- train_x %>% data.frame()
# test_x <- test_x %>% data.frame()
# your code here
sqrt(nrow(train_x))
model_knn <- knn(train = train_x, test = test_x, cl = train_y, k = 75)
dim(train_x)
dim(train_y)
dim(train_x)
dim(train_y)
dim(test_x)
dim(train_x)
dim(train_y)
dim(test_x)
train_y
train_x
model_knn <- knn(train = train_x, test = test_x, cl = train_y, k = 75)
dim(train_x)
dim(train_y)
dim(test_x)
turnover <- read.csv("turnover_balance.csv")
# your code here
turnover <- turnover %>%
mutate(Work_accident = as.factor(Work_accident),
left = as.factor(left),
promotion_last_5years = as.factor(promotion_last_5years))
# your code here
turnover %>%
filter(left == 1) %>%
group_by(division) %>%
summarise(mean_monthly = mean(average_montly_hours)) %>%
arrange(desc(mean_monthly))
set.seed(100)
# your code here
idx <- sample(nrow(turnover), nrow(turnover) * 0.8)
train <- turnover[idx,]
test <- turnover[-idx,]
model_logistic <- glm(left ~ ., data = train, family = "binomial")
# predictor variables in `train`
train_x <- train %>%
select_if(is.numeric)
# predictor variables in `test`
test_x <- test %>%
select_if(is.numeric)
# target variable in `train`
train_y <- train %>%
select(left)
# target variable in `test`
test_y <- test %>%
select(left)
# your code here
# scale train_x data
train_x <- scale(x = train_x)
# scale test_x data
test_x <- scale(x = test_x,
center = attr(train_x, "scaled:center"),
scale = attr(train_x, "scaled:scale"))
# train_x <- train_x %>% data.frame()
# test_x <- test_x %>% data.frame()
model_knn <- knn(train = train_x, test = test_x, cl = train_y, k = 75)
model_knn <- knn(train = train_x, test = test_x, cl = train_y, k = 75)
knitr::opts_chunk$set(echo = TRUE)
# your code here
turnover <- read.csv("turnover_balance.csv")
library(dplyr)
# your code here
turnover <- turnover %>%
mutate(Work_accident = as.factor(Work_accident),
left = as.factor(left),
promotion_last_5years = as.factor(promotion_last_5years))
# your code here
turnover %>%
filter(left == 1) %>%
group_by(division) %>%
summarise(mean_monthly = mean(average_montly_hours)) %>%
arrange(desc(mean_monthly))
# your code here
prop.table(table(turnover$left))
set.seed(100)
# your code here
idx <- sample(nrow(turnover), nrow(turnover) * 0.8)
train <- turnover[idx,]
test <- turnover[-idx,]
# your code here
prop.table(table(train$left))
model_logistic <- glm(left ~ ., data = train, family = "binomial")
# your code here
summary(model_logistic)
# odds ratio workaccident
exp(-1.5683412)
# predictor variables in `train`
train_x <- train %>%
select_if(is.numeric, scale())
select_if(is.numeric, scale
# predictor variables in `train`
train_x <- train %>%
# predictor variables in `train`
train_x <- train %>%
select_if(is.numeric, scale)
model_knn <- knn(train = train_x, test = test_x, cl = train_y$left, k = 75)
dim(train_y)
head(train)
turnover <- read.csv("turnover_balance.csv")
set.seed(100)
# your code here
idx <- sample(nrow(turnover), nrow(turnover) * 0.8)
train <- turnover[idx,]
test <- turnover[-idx,]
# predictor variables in `train`
train_x <- train %>%
select_if(is.numeric)
# predictor variables in `test`
test_x <- test %>%
select_if(is.numeric)
# target variable in `train`
train_y <- train %>%
select(left)
# target variable in `test`
test_y <- test %>%
select(left)
# your code here
# scale train_x data
#train_x <- scale(x = train_x)
# scale test_x data
test_x <- scale(x = test_x,
center = attr(train_x, "scaled:center"),
scale = attr(train_x, "scaled:scale"))
test_x
# scale train_x data
train_x <- scale(x = train_x)
# scale test_x data
test_x <- scale(x = test_x,
center = attr(train_x, "scaled:center"),
scale = attr(train_x, "scaled:scale"))
sqrt(nrow(train_x))
model_knn <- knn(train = train_x, test = test_x, cl = train_y$left, k = 75)
prob_value <- predict(model_logistic, newdata = test, type = "response")
model_logistic <- glm(left ~ ., data = train, family = "binomial")
summary(model_logistic)
# odds ratio workaccident
exp(-1.5683412)
# scale test_x data
test_x <- scale(x = test_x,
center = attr(train_x, "scaled:center"),
scale = attr(train_x, "scaled:scale"))
model_knn <- knn(train = train_x, test = test_x, cl = train_y$left, k = 75)
confusionMatrix(data = as.factor(model_knn), reference = test_y$left, positive = "1")
prob_value <- predict(model_logistic, newdata = test, type = "response")
pred_value <- ifelse(prob_value > 0.45, 1, 0)
table(pred_value)
# your code here
library(caret)
confusionMatrix(data = as.factor(pred_value), reference = as.factor(test$left), positive = "1")
confusionMatrix(data = as.factor(model_knn), reference = test_y$left, positive = "1")
model_knn
confusionMatrix(data = as.factor(model_knn), reference = as.factor(test_y$left), positive = "1")
options(error = recover)
devtools::install_github("gadenbuie/rsthemes")
rsthemes::try_rsthemes("light")
library(rsthemes)
knitr::opts_chunk$set(echo = TRUE)
card <- c("silver","gold","platinum")
card[2]
library(Rcpp)
install.packages("Rcpp")
install.packages("Rcpp")
library(Rcpp)
install.packages("Rcpp")
install.packages("Rcpp")
library(Rcpp)
install.packages("leaflet.extras")
options(error = recover)
install.packages("pdftools")
library(bookdown)
setwd("D:/machinelearning-FAQ")
render_book("book")
library(factoextra)
render_book("book")
render_book("book")
<<<<<<< HEAD
bookdown::render_book("book")
bookdown::render_book("book")
bookdown::render_book("book")
=======
options(error = recover)
options(error = recover)
>>>>>>> 7e10dc79dd5972cd5f902db9c0686fbb6cc284a1
options(error = recoer)
options(error = recover)
library(bookdown)
render_book("book")
options(error = recover)
