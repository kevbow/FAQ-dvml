# Neural Network and Deep Learning

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)

# scientific notation
options(scipen = 9999)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(rsample)
library(recipes)
library(keras)
```

## FAQ

1. Berapa jumlah hidden layer dan nodes untuk setiap hidden layer secara best practice dalam membangun arsitektur neural network (ANN) ?

  -  Kebanyakan orang menggunakan minimal 2 hidden layer, namun tidak menutup kemungkinan menggunakan lebih dari 2 ataupun kurang dari 2 hidden layer.
  -  Jumlah nodes biasanya semakin mengecil ketika hidden layers semakin dekat dengan output layer. Tujuannya adalah untuk melihat fitur dengan lebih spesifik. 
  -  Kebanyakan orang menggunakan angka biner $2^{n}$ seperti 1, 2, 4, 8, 16, 32, 64, 128, 256, dst karena neural network merupakan metode yang berasal dari orang computer science dan mathematics yang biasa menggunakan angka biner.

2. Fungsi aktivasi apa yang sering digunakan ketika membuat arsitektur neural network ?

  -  Pada hidden layer biasa digunakan fungsi aktivasi `relu` karena `relu` melakukan transformasi data dengan mengubah nilai negatif menjadi 0 dan membiarkan nilai positif, sehingga semakin ke belakang informasi yang dibawa tidak banyak berkurang.
  -  Pada output layer: jika casenya adalah regresi digunakan fungsi aktivasi `linear`, jika casenya adalah klasifikasi biner digunakan fungsi aktivasi `sigmoid`, dan jika casenya adalah klasifikasi multiclass digunakan fungsi aktivasi `softmax`.

3. Bagaimana menentukan batch size dan jumlah epoch ?

  -  Batch size biasanya menggunakan angka yang dapat membagi habis jumlah data, supaya data yang tersedia dapat digunakan secara keseluruhan (tidak ada yang tidak terpakai). Contoh:
Jika data train terdiri dari 800 observasi, kita bisa menggunakan batch size 200 yang dapat membagi habis 80 observasi.
  -  Jumlah epoch dimulai dari angka yang kecil terlebih dahulu supaya komputasi yang dilakukan tidak terlalu lama, kemudian dilihat apakah error dan accuracy yang dhasilkan sudah konvergen atau belum. Jika belum bisa menambahkan jumlah epoch sedikit demi sedikit, dan sebaliknya.

4. Bagaimana menentukan learning rate yang tepat ?

   Learning rate berfungsi mempercepat atau memperlambat besaran update error.

  -  Semakin besar learning rate, maka error/accuracy akan semakin cepat konvergen. Namun, bisa saja titik error paling minimum (global optimum) terlewat.
  -  Semakin kecil learning rate, maka terdapat kemungkinan yang lebih besar untuk sampai di titik error paling minimum (global optimum). Namun, error/accuracy akan lebih lama konvergen.

5. Optimizer apa yang paling sering digunakan ?

   Optimizer merupakan fungsi yang digunakan untuk mengoptimumkan error (memperkecil error). Secara sederhana untuk mengoptimumkan suatu fungsi bisa melalui fungsi turunan, pada neural network disebut `sgd.` Namun, `sgd` memiliki beberapa kekurangan sehingga mulai banyak yang memperbaiki fungsi `sgd` tersebut. Untuk sekarang ini salah satu optimizer yang cukup terkenal adalah `adam` sebagai optimizer yang merupakan perbaikan dari `sgd` karena optimizer tersebut dapat mengupdate/menyesuaikan momentum ketika proses optimisasi. Berikut link eksternal yang dapat dijadikan sebagai bahan referensi [Adaptive Moment Estimation (Adam)](https://ruder.io/optimizing-gradient-descent/index.html#adam)

   Selain tips di atas berikut link eksternal yang dapat dijadikan referensi dalam membangun arsitektur neural network [Rules-of-thumb for building a Neural Network](https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af)

6. Perbedaan metode-metode machine learning dengan neural network dan deep learning ?

  -  Neural network bukan merupakan metode yang berasal dari orang statistik melainkan lahir dari pemikiran orang-orang computer science dan math.
  -  Neural network merupakan salah satu metode machine learning, neural network yang arsitekturnya sudah cukup rumit sering disebut sebagai deep learning. Neural network memilki `1` hidden layer, sementara deep learning memiliki `> 1` hidden layer.

   Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi [Deep learning & Machine learning: whatâ€™s the difference?](https://parsers.me/deep-learning-machine-learning-whats-the-difference/)

7. Bagaimana mentransformasikan prediktor data kategorik menjadi variabel dummy?

Kita akan menggunakan data `attrition` yang memiliki variabel kategorik untuk dilakukan dummy transformation sebelum menggunakan metode neural network.
```{r}
attrition <- read.csv("data/06-NN/attrition.csv")

head(attrition, 5)
```

Kita akan melakukan cross validation, yaitu membagi data menjadi **training set** untuk proses pemodelan dan **testing set** untuk melakukan evaluasi. Namun, data train dan data test tidak langsung dimasukkan ke dalam sebuah objek melainkan dilakukan tahapan data preparation terlebih dahulu yang di dalamnya terdapat tahapan dummy transformation.

Cross validation akan dilakukan dengan menggunakan fungsi `initial_split()` dari library `rsample`. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode **stratified random sampling**, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set.
```{r}
set.seed(100)

splitted <- initial_split(attrition, prop = 0.8, strata = "attrition")

splitted
```

Melakukan tahapan data preparation yang didalamnya termasuk melakukan dummy ransformation. Data preparation yang akan dilakukan adalah menghapus variabel yang dianggap tidak berpengaruh, membuang variabel yang variansinya mendekati 0 (tidak informatif), melakukan scaling, dan melakukan dummy transformation. Proses yang dilakukan pada tahapan data preparation akan dilakukan dengan menggunakan fungsi dari library `recipies`, yaitu `step_rm()` untuk menghapus variabel, `step_nzv()` untuk membuang variabel yang variansinya mendekati 0,`step_center()` dan `step_scale()` untuk melakukan scaling, terakhir `step_dummy()` untuk dummy transformation.

```{r}
rec <- recipe(attrition ~ ., data = training(splitted)) %>% 
  step_rm(employee_count, employee_number) %>%
  step_nzv(all_predictors()) %>% 
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), -attrition, one_hot = FALSE) %>%
  prep()
```

Setelah mendefinisikan proses data preparation pada objek `rec`, selanjutnya proses tersebut diterapkan ke data train menggunakan fungsi `juice()` dan ke data test menggunakan fungsi `bake()` dari library `recipes`.
```{r}
data_train <- juice(rec)
data_test <- bake(rec, testing(splitted))

head(data_train, 5)
```

Setelah melakukan dummy transformation pada pediktor, data train dan test harus disesuaikan bentuknya untuk melalui proses building model dengan metode neural network. Target variabel yang bertipe kategorik akan dilakukan dummy transformation dengan menggunakan fungsi `to_categorical()` dari library keras, sementara semua prediktor akan diubah ke dalam bentuk matriks numerik.
```{r}
# menyiapkan data train
data_train_y <- to_categorical(as.numeric(data_train$attrition) - 1)

data_train_x <- data_train %>% 
  select(-attrition) %>% 
  data.matrix()

# menyiapkan data test
data_test_y <- to_categorical(as.numeric(data_test$attrition) - 1)

data_test_x <- data_test %>% 
  select(attrition) %>% 
  data.matrix()

# cek data train dan test
dim(data_train_x)
dim(data_train_y)
```

8. Ketika running model NN weight/bobot diinisialisasi secara random sehingga menyebabkan hasil yang berbeda jika dilakukan berulang kali. Bagaiamana cara mengatur/menggunakan seed pada Neural Network?

Metode neural network selalu menginisialisasi bobot/weight secara random di awal, sehingga ketika metode tersebut di running berulang kali akan memperoleh hasil yang berbeda. Untuk mengatasi hal tersebut kita dapat menggunakan seed (state random). Kita dapat menentukan seed dengan menggunakan fungsi `use_session_with_seed()` dari library `keras`.
```{r eval=FALSE}
use_session_with_seed(seed)
```

Selain menggunakan cara di atas kita juga dapat menggunakan seed dengan fungsi `initializer_random_normal()`. Berikut cara menggunakan seed dengan fungsi tersebut.
```{r eval=FALSE}
# define seed
set.seed(100)
initializer <- initializer_random_normal(seed = 100)

# use the seed when building architecture
model <- keras_model_sequential() %>% 
  layer_dense(units = ..., 
              activation = "...", 
              input_shape = c(...),
              kernel_initializer = initializer, 
              bias_initializer = initializer)
```

## Mathematics Concept

Aturan update weight:

1. Menghitung turunan parsial dari weight.

2. Berikut hal yang harus dilakukan jika:

   - Hasil turunannya Positif, maka nilai weight dikurangi.
   - Hasil turunannya negatif, maka nilai weight ditambah.
   
   Keduanya dilakukan dengan tujuan untuk mencari titik minimum/error terkecil.

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("assets/06-NN/updaterule.png")
```

**Forward Propagation**

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("assets/06-NN/nn.png")
```

Diketahui:

- $w_{11}=0.5$
- $w_{12}=1$
- $b_{11}=0.5$
- $w_{21}=0.5$
- $b_{21}=1$

1. Forward pass dari input ke hidden layer 1.

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("assets/06-NN/input-hidden.png")
```

$$h_{in}=x_1*w_{11}+x_2*w_{12}+b_{11} \\ h_{in}= 2*0.5 + 3*1 + 0.5 \\ h_{in}= 5$$

2. Transformasi nilai dengan Fungsi Aktivasi.

$$sigmoid=\frac{1}{1+e^{-x}}$$

$$h_{out}=\frac{1}{1+e^{-5}} \\ h_{out} = 0.9933$$

3. Forward pass hidden layer ke output layer.

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("assets/06-NN/hidden-output.png")
```

$$Output_{in}=0.9933*0.5+1 \\ Output_{in} = 1.4966$$

4. Transformasi nilai dengan fungsi aktivasi.

output linear, maka 

$$Output_{in} = Output_{out} = 1.4966$$

5. Hitung nilai error dengan cost function.

$$cost=\frac{1}{2}(output_{aktual} - output_{out})^2 \\ cost = \frac{1}{2}(2.5034)^2  \\ cost = 3.134$$

**Backpropagation**

1. Backward Pass dari Output ke hidden layer 1

2. Mengitung turunan parsial cost ke $w_{21}$

Chain rule

$$\frac{d_{cost}}{d_{w_{21}}}= \frac{d_{cost}}{d_{output_{out}}}* \frac{d_{output_{out}}}{d_{output_{in}}}*\frac{d_{output_{in}}}{d_{w_{21}}}$$

Hitung $\frac{d_{cost}}{d_{output_{out}}}$

$$\frac{d_{cost}}{d_{output_{out}}} = \frac{d(\frac{1}{2}(output_{actual}-output_{out})^2)}{d(output_{out})} \\ \frac{d_{cost}}{d_{output_{out}}} = -1 * 2 * \frac{1}{2}(4-1.4966) \\ \frac{d_{cost}}{d_{output_{out}}} = 1.4966 - 4 \\ \frac{d_{cost}}{d_{output_{out}}} = -2.5034$$

Hitung $\frac{d_{output_{out}}}{d_{output_{in}}}$

karena linear maka, 

$$\frac{d_{output_{out}}}{d_{output_{in}}} = 1$$

Hitung $\frac{d_{output_{in}}}{d_{w_{21}}}$

$$\frac{d_{output_{in}}}{d_{w_{21}}} = \frac{d(h_{out}*w_{21}+b_{21})}{d(w_{21})} \\ \frac{d_{output_{in}}}{d_{w_{21}}} = h_{out} \\ \frac{d_{output_{in}}}{d_{w_{21}}} = 0.9933$$

Jadi turunan parsial $\frac{d_{cost}}{d_{output_{out}}}$:

$$\frac{d_{cost}}{d_{output_{out}}} = -2.5034 * 1 * 0.9933 \\ \frac{d_{cost}}{d_{output_{out}}} = - 2.4866$$

3. turunan parsial cost ke $b_{21}$

Chain rule

$$\frac{d_{cost}}{d_{b_{21}}}= \frac{d_{cost}}{d_{output_{out}}}* \frac{d_{output_{out}}}{d_{output_{in}}}*\frac{d_{output_{in}}}{d_{b_{21}}} \\ \frac{d_{cost}}{d_{b_{21}}} = -2.5034*1*1 \\ \frac{d_{cost}}{d_{b_{21}}} = -2.5034$$

4. Update $w_{21}$

Misal, learning rate ($\alpha$) = 0.1, 

$$w_{21}^{'} = w_{21} - \alpha(\frac{d_{cost}}{d_{w_{21}}}) \\ w_{21}^{'} = 0.5 - (0.1*-2.4866) \\ w_{21}^{'} = 0.5 - (-2.4866) \\ w_{21}^{'} = 0.74866$$

5. update $b_{21}$

$$b_{21}^{'} = b_{21} - \alpha(\frac{d_{cost}}{d_{b_{21}}}) \\ b_{21}^{'} = 1 - (0.1*-2.5034) \\ w_{21}^{'} = 1 - (-2.5034) \\ w_{21}^{'} = 1.25034$$








